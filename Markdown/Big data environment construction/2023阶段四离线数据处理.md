# 2023阶段四离线数据处理

## 一、[环境](#environment)

## 二、<a id="OfflineDataProcessing">离线数据处理</a>

- ### 1、<a id="DataCleansing">数据清洗</a>

  - #### 1）、[ods数据取出昨天分区的数据直接插入dwd](#OdsToDwd)

  - #### 2）、[ods数据取出和dwd数据合并](OdsUnionDwd)

  - #### 3）、[ods数据取出和HBase数据合并](#OdsHBaseToDwd)

- ### 2、[dwd层数据处理计算并将结果存入MySQL](#DwdDataDepositMySQL)

- ### 3、[dwd层数据处理计算并将结果存入HBase](#DwdDataDepositHBase)

- ### 4、[dwd层数据处理计算并将结果存入ClickHouse](#DwdDataDepositClickHouse)



## 一、 <a id="environment">环境</a>

## 二、[离线数据处理](#OfflineDataProcessing)

## 1、[数据清洗](DataCleansing)

- ### 1）、<a id="OdsToDwd">ods数据取出昨天分区的数据直接插入dwd</a>

  - 思路：ods数据取出昨天分区的数据直接插入dwd对应表中(customer_balance_log、customer_login_log、customer_point_log)

    ````
    /**
         * todo 第一种操作
         * ods数据取出昨天分区的数据直接插入dwd对应表中(customer_balance_log、customer_login_log、customer_point_log)
         * */
        // 定义数组
        val tables_1 = Array("customer_balance_log", "customer_login_log", "customer_point_log")
        tables_1.foreach(table => {
          // 从ods层拿出分区数据
          val df = spark.sql(s"select * from 2023_ods1_ds_db01.${table} where etl_date='20230828'")
          // 追加模式写入hive的dwd层，以etl_date为静态分区字段
          df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_dwd1_ds_db01.fact_${table}")
        })
    ````

- ### 2）、<a id="OdsUnionDwd">ods数据取出和dwd数据合并</a> 

  - 思路：先将ods和dwd合并，根据相同id将dwd_insert_time都改为最小的那个时间，按同id的modified_time降序排名seq列，保留排名seq为1的行，删除seq列

    ````
    /*
        * todo 第二种操作
        *  ods中表数据取出和dwd对应表数据取出进行“合并”操作
        *  合并:按时间取最新的一条数据，dwd_insert_time取最早的时间，dwd_modified_time取当前时间，
        *  其他列取(customer_inf、product_info、coupon_info)
        * */
        // 获取当前时间
        val currDate = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date)
        val tables_2_ods = Array("customer_inf", "product_info", "coupon_info")
        // 分别对应需要合并的id字段
        val tables_2_id = Array("customer_id", "product_id", "coupon_id")
        for (i <- 0 until tables_2_ods.length) {
          // 取出当前需要操作的表名
          val table = tables_2_ods(i)
          // 从hive的ods层拿出数据，增加4列的目的是为了ods_df的结构和dwd_df的结构一致，然后才能union
          val ods_df = spark.sql(s"select * from 2023_ods1_ds_db01.${table} where etl_date='20230828'")
            .withColumn("dwd_insert_user", lit("user1"))
            .withColumn("dwd_insert_time", lit(currDate).cast("timestamp"))
            .withColumn("dwd_modify_user", lit("user1"))
            .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
          // 从hive的dwd层中拿出数据
          val dwd_df = spark.sql(s"select * from 2023_dwd1_ds_db01.dim_${table}")
          ods_df.printSchema()
          dwd_df.printSchema()
          val all_df = ods_df.unionByName(dwd_df)
            // 按id取相同id中dwd_insert_time最小的时间
            .withColumn("dwd_insert_time", min("dwd_insert_time").over(Window.partitionBy(tables_2_id(i))))
            //dwd_modify_time取最新的时间
            .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
            .withColumn("seq", row_number().over(Window.partitionBy(tables_2_id(i)).orderBy(desc("modified_time"))))
            .filter(_.getAs("seq").equals(1))
            .drop("seq")
          // 思路：先将ods和dwd合并，根据相同id将dwd_insert_time都改为最小的那个时间，按同id的modified_time降序排名seq列，保留排名seq为1的行，删除seq列
          all_df.write.mode(SaveMode.Overwrite).format("hive").partitionBy("etl_date").saveAsTable(s"2023_dwd1_ds_db01.dim_${table}") //存在一个读写的问题
          // 第二种方式写入数据
          //      all_df.createOrReplaceTempView("mytable")
          // 使用insert语句覆盖写入表的时候不存在读写的问题
          //      spark.sql(s"insert overwrite 2023_dwd1_ds_db01.dim_${table} select `(etl_date)?+.+`,etl_date from mytable")
        }
    ````

    

- ### 3）、<a id="OdsHBaseToDwd">ods数据取出和HBase数据合并</a>

  - 思路：取出ods中最新分区的数据dataframe，取出hbase中的数据组装成dataframe（类型的问题要注意）(增加了5列，分别是etl_date、dwd_insert_user等几列)，将ods的dataframe和hbase的dataframe进行合并union，将合并之后的dataframe追加到dwd对应的表中

    ```
    import java.text.SimpleDateFormat
    import java.util.Date
    import org.apache.hadoop.hbase.client.{ConnectionFactory, Scan}
    import org.apache.hadoop.hbase.util.Bytes
    import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
    import org.apache.spark.sql.{SaveMode, SparkSession}
    import scala.collection.convert.ImplicitConversions.`iterator asScala`
    import org.apache.spark.sql.functions._
    
    //1、取出ods中最新分区的数据dataframe
    //2、取出hbase中的数据组装成dataframe（类型的问题要注意）(增加了5列，分别是etl_date、dwd_insert_user等几列)
    //3、将ods的dataframe和hbase的dataframe进行合并union
    //4、将合并之后的dataframe追加到dwd对应的表中
    // 创建sparksession对象
        val spark = SparkSession.builder().appName("IncrementalExtraction")
          .enableHiveSupport() // 开启hive支持
          .config("hive.exec.dynamic.partition", "true") // 开启动态分区
          .config("hive.exec.dynamic.partition.mode", "nonstrict") // 设置分区的模式是非严格模式
          .config("hive.exec.max.dynamic.partitions", 2000) // 设置分区的数量
          .config("spark.sql.parser.quotedRegexColumnNames", "true") // 允许在用引号引起来的列名称中使用正则表达式
          .getOrCreate()
    
        // 隐式转换
        import spark.implicits._
    
        //获取hbase的连接对象
        val hbaseConf = HBaseConfiguration.create()
        hbaseConf.set("hbase.zookeeper.quorum", "192.168.44.61,192.168.44.62,192.168.44.63")
        // zookeeper端口号
        hbaseConf.set("hbase.zookeeper.property.clientPort", "2181")
        //获取和hbase的链接
        val connection = ConnectionFactory.createConnection(hbaseConf)
    
    
        /*
       * todo 第三种操作
       *  ods中表数据取出和hbase对应表数据取出union连接，写入dwd中
       *  注意:从hbase中取出数据的时候，按给予的hbase表的列类型取出数据，否则取出来是乱码(order_master、order_detail、product_browse)
       * */
        //拿出ods中前一个最新分区(增量)的数据
        val ods_df = spark.sql(s"select * from 2023_ods1_ds_db01.product_browse where etl_date = '20230828'")
        //从hbase中拿出数据
        val table = connection.getTable(TableName.valueOf("dsj:product_browse")) //获取表对象
        //创建查询数据的方式(比赛的时候还要加条件的)
        val scan = new Scan()
        //执行查询
        val resultScanner = table.getScanner(scan)
        val hbase_df = resultScanner.iterator().map(result => {
          val rowkey = Bytes.toString(result.getRow) //这里的rowkey暂时没用上，比赛看情况
          /*
          * 下面就是根据列族、列名取出对应的Cell单元格的数据(特别注意：比赛的时候会给一个hbase的表结构，列是什么类型就转为什么类型)
          *  如果hbase中列是int类型，则使用Bytes.toInt()
          *  如果hbase中列是double类型，则使用Bytes.toDouble()
          *  如果hbase中列是字符串或timestamp类型，则使用Bytes.toString，但是最后datafrmae需要单独处理这个为timestamp类型,下面代码有这个处理
          *  .withColumn("modified_time",col("modified_time").cast("timestamp"))
          * */
          val log_id = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("log_id"))).toInt
          val product_id = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_id"))).toInt
          val customer_id = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("customer_id"))).toInt
          val gen_order = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("gen_order"))).toInt
          val order_sn = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("order_sn")))
          val modified_time = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("modified_time")))
          (log_id, product_id, customer_id, gen_order, order_sn, modified_time) //将数据组合为元组
        }).toList.toDF("log_id", "product_id", "customer_id", "gen_order", "order_sn", "modified_time") //将列名对应上
          .withColumn("etl_date", lit("20230828")) //新增一个etl_date，目的是为了和ods结构保持一致
          .withColumn("modified_time", col("modified_time").cast("timestamp")) //将日期处理为timestamp格式
        // 获取当前时间
        val currDate = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date)
        // 将ods_df和hbase_df进行union合并（合并的前提：两个dataframe的结构完全一致）
        val ods_hbase_df = ods_df.union(hbase_df) //合并两个数据
          .withColumn("dwd_insert_user", lit("user1")) //因为dwd表中是有这4列的，所以这里新增了4列
          .withColumn("dwd_insert_time", lit(currDate).cast("timestamp"))
          .withColumn("dwd_modify_user", lit("user1"))
          .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
        // 将数据追加写入到dwd中对应表中 fact开头的表都是追加写入
        ods_hbase_df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_dwd1_ds_db01.fact_product_browse")
    
    
        //1、取出ods中最新分区的数据dataframe
        //2、取出hbase中的数据组装成dataframe（类型的问题要注意）(增加了5列，分别是etl_date、dwd_insert_user等几列)
        //3、将ods的dataframe和hbase的dataframe进行合并union
        //4、将合并之后的dataframe追加到dwd对应的表中
        //操作order_detail表
        val ods_detail_df = spark.sql("select * from 2023_ods1_ds_db01.order_detail where etl_date='20230828'")
        val detailTable = connection.getTable(TableName.valueOf("dsj:order_detail")) // 获取表对象
        val detailScanner = detailTable.getScanner(scan) // 执行查询
        val hbaseDetailDf = detailScanner.iterator().map(x => {
          val order_detail_id = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("order_detail_id")))
          val order_sn = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("order_sn")))
          val product_id = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_id")))
          val product_name = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_name")))
          val product_cnt = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_cnt")))
          val product_price = Bytes.toDouble(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_price")))
          val average_cost = Bytes.toDouble(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("average_cost")))
          val weight = Bytes.toFloat(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("weight")))
          val fee_money = Bytes.toDouble(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("fee_money")))
          val w_id = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("w_id")))
          val create_time = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("create_time")))
          val modified_time = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("modified_time")))
          (order_detail_id, order_sn, product_id, product_name, product_cnt, product_price, average_cost, weight, fee_money, w_id, create_time, modified_time)
        }).toList.toDF("order_detail_id", "order_sn", "product_id", "product_name", "product_cnt", "product_price", "average_cost", "weight", "fee_money", "w_id", "create_time", "modified_time")
          .withColumn("etl_date", lit("20230828"))
          .withColumn("modified_time", col("modified_time").cast("timestamp"))
        val detail_all_df = ods_detail_df.unionByName(hbaseDetailDf)
          .withColumn("dwd_insert_user", lit("user1")) // 因为dwd表中是有这4列的，所以这里新增了4列
          .withColumn("dwd_insert_time", lit(currDate).cast("timestamp"))
          .withColumn("dwd_modify_user", lit("user1"))
          .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
        // 追加模式写入hive，以etl_date为静态分区字段
        detail_all_df.write.mode(SaveMode.Append).format("hive").saveAsTable("2023_dwd1_ds_db01.fact_order_detail")
        spark.stop()
    
    
    
    ```

  - 第二种方法：

    ````
    package com.wzkj.qx
    
    import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
    import org.apache.hadoop.hbase.client.{ConnectionFactory, Scan}
    import org.apache.hadoop.hbase.util.Bytes
    import org.apache.spark.sql.{DataFrame, Row, SparkSession}
    import org.apache.spark.sql.functions._
    
    import java.sql.Timestamp
    import java.text.SimpleDateFormat
    import java.util.Date
    import scala.collection.convert.ImplicitConversions.`iterator asScala`
    
    object Demo1 {
      def main(args: Array[String]): Unit = {
    
    
        val sparkSession = SparkSession.builder().appName("readHbase").enableHiveSupport()
          .config("hive.exec.dynamic.partition", "true")
          .config("hive.exec.dynamic.parition.mode", "nonstrict")
    
          .getOrCreate()
        sparkSession.conf.set("spark.sql.parquet.writeLegacyFormat",true)
        sparkSession.sparkContext.setLogLevel("error")
    
        val table=Array("order_master","order_detail","product_info")
        val dwd_table=Array("fact_order_master","fact_order_detail","dim_product_info")
        val date=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())
        for (i<-0 until(table.length)){
    
          val ods_df = sparkSession.sql(s"select * from ods.${table(i)} where etl_date='20230427'")
            .drop("etl_date")
          var hbase_df = readHbaseDf(sparkSession, ods_df, table(i))
    
          hbase_df.show()
          var final_df = hbase_df.unionByName(ods_df)
            .withColumn("dwd_insert_user", lit("user1"))
            .withColumn("dwd_insert_time", lit(date).cast("timestamp"))
            .withColumn("dwd_modify_user", lit("user1"))
            .withColumn("dwd_modify_time", lit(date).cast("timestamp"))
    
          if (table(i).equals("order_master")){
            final_df=final_df.where(length(col("city")).<=(lit(8)))
              .withColumn("create_time",to_timestamp(col("create_time"),"yyyyMMddHHmmss"))
              .withColumn("shipping_time",to_timestamp(col("shipping_time"),"yyyyMMddHHmmss"))
              .withColumn("receive_time",to_timestamp(col("receive_time"),"yyyyMMddHHmmss"))
              .withColumn("pay_time",to_timestamp(col("pay_time"),"yyyyMMddHHmmss"))
    
          }else if (table(i).equals("order_detail")){
            final_df=final_df
              .withColumn("create_time",to_timestamp(col("create_time"),"yyyyMMddHHmmss"))
    
          }
          final_df.createOrReplaceTempView("newtable")
    
          final_df.show()
          println("ods数量"+ods_df.count())
          println("hbase数量"+hbase_df.count())
          println("hbase合并后过滤ods数量"+final_df.count())
    
          sparkSession.sql(s"insert overwrite dwd.${dwd_table(i)} partition(etl_date='20230427') select * from newtable")
    
          println(dwd_table(i)+"插入成功")
          println("_____________________________________")
        }
        sparkSession.stop()
      }
      def readHbaseDf(sparkSession:SparkSession,df:DataFrame,name:String): DataFrame ={
        val fields = df.schema.fields
        val config = HBaseConfiguration.create()
        config.set("habse.zookeeper.quorum","master:2181,slave1:2181,salve2:2181")
        val conn = ConnectionFactory.createConnection(config)
        val table = conn.getTable(TableName.valueOf("hbase_" + name))
        val scan = new Scan()
        val list= table.getScanner(scan).iterator().filter(x=>{
    
          val rowkey = Bytes.toString(x.getRow)
          val str = rowkey.substring(1, 9)
          val bool=str.contains("20221001")
    
          bool
        }).map(x=>{
    
          val values=fields.map(field=>{
    
            val col = field.name
    
            val value=field.dataType.typeName match {
              case "long"=>Bytes.toLong(x.getValue("info".getBytes(),col.getBytes()))
              case "double"=>Bytes.toBigDecimal(x.getValue("info".getBytes(),col.getBytes())).doubleValue()
              case "float"=>Bytes.toFloat(x.getValue("info".getBytes(),col.getBytes()))
              case "integer"=>Bytes.toInt(x.getValue("info".getBytes(),col.getBytes()))
              case "timestamp"=>Timestamp.valueOf(Bytes.toString(x.getValue("info".getBytes(),col.getBytes())))
              case typestr if typestr.contains("decimal") =>Bytes.toBigDecimal(x.getValue("info".getBytes(),col.getBytes()))
              case _=>Bytes.toString(x.getValue("info".getBytes(),col.getBytes()))
            }
            value
          })
          Row(values:_*)
        }).toList
        val df2 = sparkSession.createDataFrame(sparkSession.sparkContext.makeRDD(list), df.schema)
        df2
      }
    }
    ````

    

### 2、<a id="DwdDataDepositMySQL">dwd层数据处理计算并将结果存入MySQL</a>

### 3、<a id="DwdDataDepositHBase">dwd层数据处理计算并将结果存入HBase</a>

### 4、<a id="DwdDataDepositClickHouse">dwd层数据处理计算并将结果存入ClickHouse</a>
