# 2023阶段四离线数据处理

## 一、[环境](#environment)

## 二、<a id="OfflineDataProcessing">离线数据处理</a>

- ### 1、<a id="DataCleansing">数据清洗</a>

  - #### 1）、[ods数据取出昨天分区的数据直接插入dwd](#OdsToDwd)

  - #### 2）、[ods数据取出和dwd数据合并](OdsUnionDwd)

  - #### 3）、[ods数据取出和HBase数据合并](#OdsHBaseToDwd)

- ### 2、[dwd层数据处理计算并将结果存入MySQL](#DwdDataDepositMySQL)

- ### 3、[dwd层数据处理计算并将结果存入HBase](#DwdDataDepositHBase)

- ### 4、[dwd层数据处理计算并将结果存入ClickHouse](#DwdDataDepositClickHouse)



## 一、 <a id="environment">环境</a>

## 二、[离线数据处理](#OfflineDataProcessing)

## 1、[数据清洗](DataCleansing)

- ### 1）、<a id="OdsToDwd">ods数据取出昨天分区的数据直接插入dwd</a>

- ### 1、hive常用语法

  - #### 1、查看数据库

    ```
    show databases;
    ```

  - #### 2、创建数据库

    ```
    create databases 数据库名
    ```

  - #### 3、进入数据库

    ````
    use 数据库名
    ````

  - #### 4、创建分区表

    ```
    
    create table dim_brand_info(brand_id int,brand_name string,telephone string,brand_web string,
    brand_logo string,brand_desc string,brand_status int,brand_order int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_coupon_info(coupon_id int,coupon_name string,coupon_type int,condition_amount int,condition_num int,
    activity_id string,benefit_amount decimal(8,2),benefit_discount decimal(8,2),modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_coupon_use(coupon_use_id int,coupon_id int,customer_id int,order_sn string,coupon_status string,
    get_time timestamp,used_time timestamp,pay_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_addr(customer_addr_id int,customer_id int,zip int,province string,city string,
    address string,is_default int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_balance_log(balance_id int,customer_id int,source int,
    source_sn string,create_time timestamp,amount decimal(8,2),
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_inf(customer_inf_id int,customer_id int,customer_name string,
    identity_card_type int,identity_card_no string,mobile_phone string,
    customer_email string,gender string,customer_point int,register_time timestamp,birthday timestamp,
    customer_level int,customer_money double,modified_time timestamp,dwd_insert_user string,
    dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp) partitioned by (etl_date string) 
    row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_level_inf(customer_level int,level_name string,min_point int,max_point int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_login(customer_id int,login_name string,password string,user_stats int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_login_log(login_id int,customer_id int,login_time timestamp,login_ip string,login_type int,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table dim_customer_point_log(point_id int,customer_id int,source int,refer_number string,change_point int,create_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    
    create table dim_favor_info(favor_id int,customer_id int,product_id int,is_cancel int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    
    create table fact_order_cart(cart_id int,customer_id int,product_id int,product_amount int,
    price decimal(8,2),add_time timestamp,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    
    create table fact_order_detail(order_detail_id int,order_sn string,product_id int,product_name string,
    product_cnt int,product_price double,average_cost double,weight float,fee_money double,w_id int,
    create_time timestamp,modified_time timestamp,dwd_insert_user string,dwd_insert_time string,
    dwd_modify_user string,dwd_modify_time string) partitioned by (etl_date String) 
    row format delimited fields terminated by '\t'  lines terminated by '\n';
    
    
    
    create table fact_order_master(order_id int,order_sn string,customer_id int,shipping_user string,province string,
    city string,address string,order_source int,payment_method int,order_money double,district_money double,
    shipping_money double,payment_money double,shipping_comp_name string,shipping_sn string,
    create_time timestamp,shipping_time timestamp,pay_time timestamp,receive_time timestamp,order_status string,
    order_point int,invoice_time string,modified_time timestamp,dwd_insert_user string,dwd_insert_time timestamp,
    dwd_modify_user string,dwd_modify_time timestamp) partitioned by(etl_date string) 
    row format  delimited fields terminated by '\t' lines terminated by '\n';
    
    
    
    create table fact_product_browse(log_id int,product_id int,customer_id int,gen_order int,order_sn string,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    
    create table fact_product_category(category_id int,category_name string,category_code string,
    parent_id int,category_level int,category_status int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string) row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table fact_product_comment(comment_id int,product_id int,order_sn string,customer_id int,
    title string,content string,audit_status int,audit_time timestamp,product_point int,shipping_point int,
    modified_time timestamp,dwd_insert_user string,dwd_insert_time string,
    dwd_modify_user string,dwd_modify_time string) partitioned by (etl_date String) 
    row format delimited fields terminated by '\t'  lines terminated by '\n';
    
    
    create table dim_product_info(product_id int,product_core string,product_name string,bar_code string,
    brand_id int,one_category_id int,two_category_id int,three_category_id int,supplier_id int,price double,
    average_cost double,publish_status int,audit_status int,weight double,length double,height double,width double,
    color_type string,production_date timestamp,shelf_life int,descript string,indate timestamp,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date string)  row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table fact_product_pic_info(product_pic_id int,product_id int,pic_desc string,pic_url string,
    is_master int,pic_order int,pic_status int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date String)  row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table fact_shipping_info(ship_id int,ship_name string,ship_contact string,telephone string,price decimal(8,2),modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date String)  row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table fact_supplier_info(supplier_id int,supplier_code string,supplier_name string,supplier_type int,link_man string,
    phone_number string,bank_name string,bank_account string,address string,supplier_status int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date String)  row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table fact_warehouse_info(w_id int,warehouse_sn string,warehoust_name string,warehouse_phone string,contact string,
    province string,city string,address string,warehouse_status int,modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date String)  row format delimited fields terminated by '\t' lines terminated by '\n';
    
    
    create table fact_warehouse_product(wp_id int,product_id int,w_id int,current_cnt int,lock_cnt int,in_transit_cnt int,
    average_cost decimal(8,2),modified_time timestamp,
    dwd_insert_user string,dwd_insert_time timestamp,dwd_modify_user string,dwd_modify_time timestamp)
    partitioned by (etl_date String)  row format delimited fields terminated by '\t' lines terminated by '\n';
    ```

  - #### 5、查看表结构

    - 查看表结构

      ```
      DESCRIBE 表名
      ```

    - 查看表的列信息

      ```
      desc 表名;
      ```

    - 查看更加详细的表信息

      ```
      desc extended tb_class; 
      ```

    - 查看建表语句

      ```
      show create table tb_class;
      ```

    - 查看hive中所有的函数

      ```
      show functions;
      ```

  - #### 6、删除表

    ```
    drop table 表名
    ```

  - #### 7、清除数据

    ```
    truncate table 表名;  【不能删除表中的分区，只能删除表中的数据】
    ```

  - #### 8、删除分区数据

    ```
    alter table 表名 drop partition(分区列=分区列的值)
    alter table 表名 drop partition(分区列 <= 2022)
           【删除分区后该分区的数据也会被同步删除】
    ```

  - #### 9、查看分区数据

    ```
    show partitions 表名;
    show partitions 库名.表名;
    ```

  

  - 思路：ods数据取出昨天分区的数据直接插入dwd对应表中(customer_balance_log、customer_login_log、customer_point_log)

    ````
    /**
         * todo 第一种操作
         * ods数据取出昨天分区的数据直接插入dwd对应表中(customer_balance_log、customer_login_log、customer_point_log)
         * */
        // 定义数组
        val tables_1 = Array("customer_balance_log", "customer_login_log", "customer_point_log")
        tables_1.foreach(table => {
          // 从ods层拿出分区数据
          val df = spark.sql(s"select * from 2023_ods1_ds_db01.${table} where etl_date='20230828'")
          // 追加模式写入hive的dwd层，以etl_date为静态分区字段
          df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_dwd1_ds_db01.fact_${table}")
        })
    ````

- ### 2）、<a id="OdsUnionDwd">ods数据取出和dwd数据合并</a> 

  - 思路：先将ods和dwd合并，根据相同id将dwd_insert_time都改为最小的那个时间，按同id的modified_time降序排名seq列，保留排名seq为1的行，删除seq列

    ````
    /*
        * todo 第二种操作
        *  ods中表数据取出和dwd对应表数据取出进行“合并”操作
        *  合并:按时间取最新的一条数据，dwd_insert_time取最早的时间，dwd_modified_time取当前时间，
        *  其他列取(customer_inf、product_info、coupon_info)
        * */
        // 获取当前时间
        val currDate = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date)
        val tables_2_ods = Array("customer_inf", "product_info", "coupon_info")
        // 分别对应需要合并的id字段
        val tables_2_id = Array("customer_id", "product_id", "coupon_id")
        for (i <- 0 until tables_2_ods.length) {
          // 取出当前需要操作的表名
          val table = tables_2_ods(i)
          // 从hive的ods层拿出数据，增加4列的目的是为了ods_df的结构和dwd_df的结构一致，然后才能union
          val ods_df = spark.sql(s"select * from 2023_ods1_ds_db01.${table} where etl_date='20230828'")
            .withColumn("dwd_insert_user", lit("user1"))
            .withColumn("dwd_insert_time", lit(currDate).cast("timestamp"))
            .withColumn("dwd_modify_user", lit("user1"))
            .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
          // 从hive的dwd层中拿出数据
          val dwd_df = spark.sql(s"select * from 2023_dwd1_ds_db01.dim_${table}")
          ods_df.printSchema()
          dwd_df.printSchema()
          val all_df = ods_df.unionByName(dwd_df)
            // 按id取相同id中dwd_insert_time最小的时间
            .withColumn("dwd_insert_time", min("dwd_insert_time").over(Window.partitionBy(tables_2_id(i))))
            //dwd_modify_time取最新的时间
            .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
            .withColumn("seq", row_number().over(Window.partitionBy(tables_2_id(i)).orderBy(desc("modified_time"))))
            .filter(_.getAs("seq").equals(1))
            .drop("seq")
          // 思路：先将ods和dwd合并，根据相同id将dwd_insert_time都改为最小的那个时间，按同id的modified_time降序排名seq列，保留排名seq为1的行，删除seq列
          all_df.write.mode(SaveMode.Overwrite).format("hive").partitionBy("etl_date").saveAsTable(s"2023_dwd1_ds_db01.dim_${table}") //存在一个读写的问题
          // 第二种方式写入数据
          //      all_df.createOrReplaceTempView("mytable")
          // 使用insert语句覆盖写入表的时候不存在读写的问题
          //      spark.sql(s"insert overwrite 2023_dwd1_ds_db01.dim_${table} select `(etl_date)?+.+`,etl_date from mytable")
        }
    ````

    

- ### 3）、<a id="OdsHBaseToDwd">ods数据取出和HBase数据合并</a>

  - 思路：取出ods中最新分区的数据dataframe，取出hbase中的数据组装成dataframe（类型的问题要注意）(增加了5列，分别是etl_date、dwd_insert_user等几列)，将ods的dataframe和hbase的dataframe进行合并union，将合并之后的dataframe追加到dwd对应的表中

    ```
    import java.text.SimpleDateFormat
    import java.util.Date
    import org.apache.hadoop.hbase.client.{ConnectionFactory, Scan}
    import org.apache.hadoop.hbase.util.Bytes
    import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
    import org.apache.spark.sql.{SaveMode, SparkSession}
    import scala.collection.convert.ImplicitConversions.`iterator asScala`
    import org.apache.spark.sql.functions._
    
    //1、取出ods中最新分区的数据dataframe
    //2、取出hbase中的数据组装成dataframe（类型的问题要注意）(增加了5列，分别是etl_date、dwd_insert_user等几列)
    //3、将ods的dataframe和hbase的dataframe进行合并union
    //4、将合并之后的dataframe追加到dwd对应的表中
    // 创建sparksession对象
        val spark = SparkSession.builder().appName("IncrementalExtraction")
          .enableHiveSupport() // 开启hive支持
          .config("hive.exec.dynamic.partition", "true") // 开启动态分区
          .config("hive.exec.dynamic.partition.mode", "nonstrict") // 设置分区的模式是非严格模式
          .config("hive.exec.max.dynamic.partitions", 2000) // 设置分区的数量
          .config("spark.sql.parser.quotedRegexColumnNames", "true") // 允许在用引号引起来的列名称中使用正则表达式
          .getOrCreate()
    
        // 隐式转换
        import spark.implicits._
    
        //获取hbase的连接对象
        val hbaseConf = HBaseConfiguration.create()
        hbaseConf.set("hbase.zookeeper.quorum", "192.168.44.61,192.168.44.62,192.168.44.63")
        // zookeeper端口号
        hbaseConf.set("hbase.zookeeper.property.clientPort", "2181")
        //获取和hbase的链接
        val connection = ConnectionFactory.createConnection(hbaseConf)
    
    
        /*
       * todo 第三种操作
       *  ods中表数据取出和hbase对应表数据取出union连接，写入dwd中
       *  注意:从hbase中取出数据的时候，按给予的hbase表的列类型取出数据，否则取出来是乱码(order_master、order_detail、product_browse)
       * */
        //拿出ods中前一个最新分区(增量)的数据
        val ods_df = spark.sql(s"select * from 2023_ods1_ds_db01.product_browse where etl_date = '20230828'")
        //从hbase中拿出数据
        val table = connection.getTable(TableName.valueOf("dsj:product_browse")) //获取表对象
        //创建查询数据的方式(比赛的时候还要加条件的)
        val scan = new Scan()
        //执行查询
        val resultScanner = table.getScanner(scan)
        val hbase_df = resultScanner.iterator().map(result => {
          val rowkey = Bytes.toString(result.getRow) //这里的rowkey暂时没用上，比赛看情况
          /*
          * 下面就是根据列族、列名取出对应的Cell单元格的数据(特别注意：比赛的时候会给一个hbase的表结构，列是什么类型就转为什么类型)
          *  如果hbase中列是int类型，则使用Bytes.toInt()
          *  如果hbase中列是double类型，则使用Bytes.toDouble()
          *  如果hbase中列是字符串或timestamp类型，则使用Bytes.toString，但是最后datafrmae需要单独处理这个为timestamp类型,下面代码有这个处理
          *  .withColumn("modified_time",col("modified_time").cast("timestamp"))
          * */
          val log_id = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("log_id"))).toInt
          val product_id = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_id"))).toInt
          val customer_id = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("customer_id"))).toInt
          val gen_order = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("gen_order"))).toInt
          val order_sn = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("order_sn")))
          val modified_time = Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("modified_time")))
          (log_id, product_id, customer_id, gen_order, order_sn, modified_time) //将数据组合为元组
        }).toList.toDF("log_id", "product_id", "customer_id", "gen_order", "order_sn", "modified_time") //将列名对应上
          .withColumn("etl_date", lit("20230828")) //新增一个etl_date，目的是为了和ods结构保持一致
          .withColumn("modified_time", col("modified_time").cast("timestamp")) //将日期处理为timestamp格式
        // 获取当前时间
        val currDate = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date)
        // 将ods_df和hbase_df进行union合并（合并的前提：两个dataframe的结构完全一致）
        val ods_hbase_df = ods_df.union(hbase_df) //合并两个数据
          .withColumn("dwd_insert_user", lit("user1")) //因为dwd表中是有这4列的，所以这里新增了4列
          .withColumn("dwd_insert_time", lit(currDate).cast("timestamp"))
          .withColumn("dwd_modify_user", lit("user1"))
          .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
        // 将数据追加写入到dwd中对应表中 fact开头的表都是追加写入
        ods_hbase_df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_dwd1_ds_db01.fact_product_browse")
    
    
        //1、取出ods中最新分区的数据dataframe
        //2、取出hbase中的数据组装成dataframe（类型的问题要注意）(增加了5列，分别是etl_date、dwd_insert_user等几列)
        //3、将ods的dataframe和hbase的dataframe进行合并union
        //4、将合并之后的dataframe追加到dwd对应的表中
        //操作order_detail表
        val ods_detail_df = spark.sql("select * from 2023_ods1_ds_db01.order_detail where etl_date='20230828'")
        val detailTable = connection.getTable(TableName.valueOf("dsj:order_detail")) // 获取表对象
        val detailScanner = detailTable.getScanner(scan) // 执行查询
        val hbaseDetailDf = detailScanner.iterator().map(x => {
          val order_detail_id = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("order_detail_id")))
          val order_sn = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("order_sn")))
          val product_id = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_id")))
          val product_name = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_name")))
          val product_cnt = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_cnt")))
          val product_price = Bytes.toDouble(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("product_price")))
          val average_cost = Bytes.toDouble(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("average_cost")))
          val weight = Bytes.toFloat(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("weight")))
          val fee_money = Bytes.toDouble(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("fee_money")))
          val w_id = Bytes.toInt(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("w_id")))
          val create_time = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("create_time")))
          val modified_time = Bytes.toString(x.getValue(Bytes.toBytes("info"), Bytes.toBytes("modified_time")))
          (order_detail_id, order_sn, product_id, product_name, product_cnt, product_price, average_cost, weight, fee_money, w_id, create_time, modified_time)
        }).toList.toDF("order_detail_id", "order_sn", "product_id", "product_name", "product_cnt", "product_price", "average_cost", "weight", "fee_money", "w_id", "create_time", "modified_time")
          .withColumn("etl_date", lit("20230828"))
          .withColumn("modified_time", col("modified_time").cast("timestamp"))
        val detail_all_df = ods_detail_df.unionByName(hbaseDetailDf)
          .withColumn("dwd_insert_user", lit("user1")) // 因为dwd表中是有这4列的，所以这里新增了4列
          .withColumn("dwd_insert_time", lit(currDate).cast("timestamp"))
          .withColumn("dwd_modify_user", lit("user1"))
          .withColumn("dwd_modify_time", lit(currDate).cast("timestamp"))
        // 追加模式写入hive，以etl_date为静态分区字段
        detail_all_df.write.mode(SaveMode.Append).format("hive").saveAsTable("2023_dwd1_ds_db01.fact_order_detail")
        spark.stop()
    
    
    
    ```

  - 第二种方法：

    ````
    package com.wzkj.qx
    
    import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
    import org.apache.hadoop.hbase.client.{ConnectionFactory, Scan}
    import org.apache.hadoop.hbase.util.Bytes
    import org.apache.spark.sql.{DataFrame, Row, SparkSession}
    import org.apache.spark.sql.functions._
    
    import java.sql.Timestamp
    import java.text.SimpleDateFormat
    import java.util.Date
    import scala.collection.convert.ImplicitConversions.`iterator asScala`
    
    object Demo1 {
      def main(args: Array[String]): Unit = {
    
    
        val sparkSession = SparkSession.builder().appName("readHbase").enableHiveSupport()
          .config("hive.exec.dynamic.partition", "true")
          .config("hive.exec.dynamic.parition.mode", "nonstrict")
    
          .getOrCreate()
        sparkSession.conf.set("spark.sql.parquet.writeLegacyFormat",true)
        sparkSession.sparkContext.setLogLevel("error")
    
        val table=Array("order_master","order_detail","product_info")
        val dwd_table=Array("fact_order_master","fact_order_detail","dim_product_info")
        val date=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())
        for (i<-0 until(table.length)){
    
          val ods_df = sparkSession.sql(s"select * from ods.${table(i)} where etl_date='20230427'")
            .drop("etl_date")
          var hbase_df = readHbaseDf(sparkSession, ods_df, table(i))
    
          hbase_df.show()
          var final_df = hbase_df.unionByName(ods_df)
            .withColumn("dwd_insert_user", lit("user1"))
            .withColumn("dwd_insert_time", lit(date).cast("timestamp"))
            .withColumn("dwd_modify_user", lit("user1"))
            .withColumn("dwd_modify_time", lit(date).cast("timestamp"))
    
          if (table(i).equals("order_master")){
            final_df=final_df.where(length(col("city")).<=(lit(8)))
              .withColumn("create_time",to_timestamp(col("create_time"),"yyyyMMddHHmmss"))
              .withColumn("shipping_time",to_timestamp(col("shipping_time"),"yyyyMMddHHmmss"))
              .withColumn("receive_time",to_timestamp(col("receive_time"),"yyyyMMddHHmmss"))
              .withColumn("pay_time",to_timestamp(col("pay_time"),"yyyyMMddHHmmss"))
    
          }else if (table(i).equals("order_detail")){
            final_df=final_df
              .withColumn("create_time",to_timestamp(col("create_time"),"yyyyMMddHHmmss"))
    
          }
          final_df.createOrReplaceTempView("newtable")
    
          final_df.show()
          println("ods数量"+ods_df.count())
          println("hbase数量"+hbase_df.count())
          println("hbase合并后过滤ods数量"+final_df.count())
    
          sparkSession.sql(s"insert overwrite dwd.${dwd_table(i)} partition(etl_date='20230427') select * from newtable")
    
          println(dwd_table(i)+"插入成功")
          println("_____________________________________")
        }
        sparkSession.stop()
      }
      def readHbaseDf(sparkSession:SparkSession,df:DataFrame,name:String): DataFrame ={
        val fields = df.schema.fields
        val config = HBaseConfiguration.create()
        config.set("habse.zookeeper.quorum","master:2181,slave1:2181,salve2:2181")
        val conn = ConnectionFactory.createConnection(config)
        val table = conn.getTable(TableName.valueOf("hbase_" + name))
        val scan = new Scan()
        val list= table.getScanner(scan).iterator().filter(x=>{
    
          val rowkey = Bytes.toString(x.getRow)
          val str = rowkey.substring(1, 9)
          val bool=str.contains("20221001")
    
          bool
        }).map(x=>{
    
          val values=fields.map(field=>{
    
            val col = field.name
    
            val value=field.dataType.typeName match {
              case "long"=>Bytes.toLong(x.getValue("info".getBytes(),col.getBytes()))
              case "double"=>Bytes.toBigDecimal(x.getValue("info".getBytes(),col.getBytes())).doubleValue()
              case "float"=>Bytes.toFloat(x.getValue("info".getBytes(),col.getBytes()))
              case "integer"=>Bytes.toInt(x.getValue("info".getBytes(),col.getBytes()))
              case "timestamp"=>Timestamp.valueOf(Bytes.toString(x.getValue("info".getBytes(),col.getBytes())))
              case typestr if typestr.contains("decimal") =>Bytes.toBigDecimal(x.getValue("info".getBytes(),col.getBytes()))
              case _=>Bytes.toString(x.getValue("info".getBytes(),col.getBytes()))
            }
            value
          })
          Row(values:_*)
        }).toList
        val df2 = sparkSession.createDataFrame(sparkSession.sparkContext.makeRDD(list), df.schema)
        df2
      }
    }
    ````

    

### 2、<a id="DwdDataDepositMySQL">dwd层数据处理计算并将结果存入MySQL</a>

### 3、<a id="DwdDataDepositHBase">dwd层数据处理计算并将结果存入HBase</a>

### 4、<a id="DwdDataDepositClickHouse">dwd层数据处理计算并将结果存入ClickHouse</a>
