# 2023阶段二数据采集

## 一、[启动环境作](#initiate)

- ### [全部启动](#all)

## 二、[环境](#environment)

## 三、[数据采集](#DataAcquisition)

- ### 1、[增量抽取](#IncrementalExtraction)

- ### 2、[#Flume端口实时采集存入Kafka](#FlumePort)

- ### 3、[Maxwell采集MySQL的binlog日志存入Kafka](#MaxwellGatherBinlog)





<br>



## 一、<a id="initiate">启动环境和基本操作</a>

### <a id="all">全部启动</a>

```
/usr/local/src/hadoop/sbin/start-all.sh


# 每台机器上分别启动
/usr/local/src/zookeeper/bin/zkServer.sh  start


service mysql restart


flume-ng agent -c conf -f /usr/local/src/flume/conf/xxxxxx.conf --name a1 -Dflume.root.logger=INFO,console


# 三台机器分别启动kafka（先保证zookeeper是正常的）
kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
# 启动生产者
kafka-console-producer.sh --broker-list bigdata1:9092 --topic 主题名
# 启动消费者
kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic 主题名 --from-beginning


/usr/local/src/hbase/bin/start-hbase.sh


# 正常启动
clickhouse-server --config-file=/etc/clickhouse-server/config.xml
# 后台启动
clickhouse-server --config-file=/etc/clickhouse-server/config.xml >null 2>&1 &
【启动之后可以通过netstat -tnulp命令查看9002端口和8123端口占用情况是否正常】



/usr/local/src/maxwell/bin/maxwell --config /usr/local/src/maxwell/config.properties
```



<br>



## 二、<a id="environment">环境</a>

<br>



## 三、<a id="DataAcquisition">数据采集</a>

### 1、<a id="IncrementalExtraction">增量抽取</a>

### 2、<a id="FlumePort">Flume端口实时采集存入Kafka</a>

#### 1）数据从什么地方来

- 有一个脚本提供，作用（读取本地mysql中ds_db01数据库的数据，取其中一部分表发送给端口，取其中一部分表插入另外一个mysql数据库中ds_realtime_db）
  - 发送给端口的数据，由flume采集到kafka中
  - 插入到ds_realtime_db中的数据，由maxwell采集到kafka中

#### 2）脚本运行的前提（启动脚本应该在flume和Maxwell都配置好后）

- 脚本文件的颜色是绿色（chmod 777 jiaoben_socket）

- 本机mysql有ds_db01库和ds_realtime_db库（本机的主机名叫bigdata1）

  - 如果你的主机名不叫bigdata1，那么可以在/etc/hosts下强行增加一个映射：192.168.44.61 bigdata1

- 启动Flume、Kafka和Maxwell

  ```
  flume-ng agent -c conf -f /usr/local/src/flume/conf/jiaoben.conf --name a1 -DFlume.root.logger=INFO,console
  
  maxwell --config /usr/local/src/maxwell/config.properties -daemon
  
  
  三台机器分别启动kafka：
  kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
  ```

#### 3）启动Kafka

- ```
  三台机器分别启动kafka：
  kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
  ```

#### 4）创建主题

- ````
  kafka-topics.sh --create --bootstrap-server bigdata1:9092 --replication-factor 1 --partitions 1 --topic ods_mall_log
  ````

#### 5）启动消费者：(启动脚本后可用于观察数据)

- ```
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning
  ```

#### 6）查看主题

- ```
  kafka-topics.sh --list --bootstrap-server bigdata1:9092
  ```

#### 7）**flume启动**并且监听了25001**端口**

- ```
  vim jiaoben.conf
  
  #定义采集器的三个组件名称
  a1.sources=r1
  a1.sinks=k1
  a1.channels=c1
  
  #数据从哪来（某个机器的端口中来）
  a1.sources.r1.type=netcat
  a1.sources.r1.bind=localhost
  a1.sources.r1.port=25001
  
  #数据缓存到内存中
  a1.channels.c1.type=memory
  
  #数据打印到控制台（到哪去）
  a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
  a1.sinks.k1.brokerList=bigdata1:9092,bigdata2:9092,bigdata3:9092
  a1.sinks.k1.topic=ods_mall_log
  
  #绑定三者关联
  a1.sources.r1.channels=c1
  a1.sinks.k1.channel=c1
  ```

#### 8）启动flume

- ```
  flume-ng agent -c conf -f /usr/local/src/flume/conf/jiaoben.conf --name a1 -DFlume.root.logger=INFO,console
  ```

### 3、<a id="MaxwellGatherBinlog">Maxwell采集MySQL的binlog日志存入Kafka</a>

#### 1）使用Maxwell采集数据库ds_realtime_db中的变化，将变化的记录(json格式)发送给kafka：

- 修改mysql让其支持写binlog日志，并且指定日志类型以及数据库设置

  ```
  vi /etc/my.cnf
  
  在原来配置最后加入：
  #数据库id
  server-id=1
  #启动binlog，该参数的值会作为binlog的文件名
  log-bin=mysql-bin
  #binlog类型，maxwell要求为row类型
  binlog_format=row
  #启用binlog的数据库，需根据实际情况作出修改
  binlog-do-db=ds_realtime_db
  ```

- 修改maxwell的配置文件（连接mysql的登录配置、数据发送给kafka的配置）

  ```
  cp config.properties.example config.properties
  vi config.properties
          
  producer=kafka
  kafka.bootstrap.servers=bigdata1:9092,bigdata2:9092,bigdata3:9092
  kafka_topic=ods_mall_data
  
  # mysql login info
  host=localhost
  user=root
  password=123456
  ```

- 注意：

  - 如果重新配置maxwell后者监听其他新建的数据库，则需要将mysql中产生的maxwell数据库给删了

#### 2）启动Kafka

- ```
  三台机器分别启动kafka：
  kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
  ```

#### 3）创建主题

- ````
  kafka-topics.sh --create --bootstrap-server bigdata1:9092 --replication-factor 1 --partitions 1 --topic ods_mall_log
  ````

#### 4）启动消费者：(启动脚本后可用于观察数据)

- ```
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning
  ```

#### 5）查看主题

- ```
  kafka-topics.sh --list --bootstrap-server bigdata1:9092
  ```

#### 6）启动maxwell

- ````
  maxwell --config /usr/local/src/maxwell/config.properties -daemon
  ````

#### 7）启动脚本

- ```
  /opt/jiaoben.socket
  ```

#### 8）查看数据

- 查看全部数据

  ```
  # 查看Flume端口采集数据到Kafka的数据
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning
  
  # 查看Maxwell采集MySQL日志（binlog）数据到Kafka的数据
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_data --from-beginning
  ```

- 查看前2条数据

  ```
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning --max-messages 2
  ```


#### 9）成功标准：两个消费者中都能看到数据

 







