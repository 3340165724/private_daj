# 2023阶段二数据采集

## 一、[启动环境作](#initiate)

- ### [全部启动](#all)

## 二、<a id="environment">环境</a>

- ### 1、[pom.xml](#pom)

- ### 2、[打包方式](#PackagingMethod)

- ### 3、[创建sparksession对象](#sparksession)

## 三、<a id="DataAcquisition">数据采集</a>

- ### 1、[增量抽取](#IncrementalExtraction)

- ### 2、[#Flume端口实时采集存入Kafka](#FlumePort)

- ### 3、[Maxwell采集MySQL的binlog日志存入Kafka](#MaxwellGatherBinlog)





<br>



## 一、<a id="initiate">启动环境和基本操作</a>

### <a id="all">全部启动</a>

```
/usr/local/src/hadoop/sbin/start-all.sh


# 每台机器上分别启动
/usr/local/src/zookeeper/bin/zkServer.sh  start


service mysql restart


flume-ng agent -c conf -f /usr/local/src/flume/conf/xxxxxx.conf --name a1 -Dflume.root.logger=INFO,console


# 三台机器分别启动kafka（先保证zookeeper是正常的）
kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
# 启动生产者
kafka-console-producer.sh --broker-list bigdata1:9092 --topic 主题名
# 启动消费者
kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic 主题名 --from-beginning


/usr/local/src/hbase/bin/start-hbase.sh


# 正常启动
clickhouse-server --config-file=/etc/clickhouse-server/config.xml
# 后台启动
clickhouse-server --config-file=/etc/clickhouse-server/config.xml >null 2>&1 &
【启动之后可以通过netstat -tnulp命令查看9002端口和8123端口占用情况是否正常】



/usr/local/src/maxwell/bin/maxwell --config /usr/local/src/maxwell/config.properties
```



<br>



## 二、[环境](#environment)

- #### <a id="pom">pom.xml</a>

  ````
   <properties>
          <maven.compiler.source>8</maven.compiler.source>
          <maven.compiler.target>8</maven.compiler.target>
          <flink.version>1.14.0</flink.version>
          <scala.version>2.12</scala.version>
          <hive.version>3.1.2</hive.version>
          <mysqlconnect.version>5.1.47</mysqlconnect.version>
          <clickhouse.version>0.3.2</clickhouse.version>
          <hdfs.version>3.1.3</hdfs.version>
          <spark.version>3.1.1</spark.version>
          <hbase.version>2.2.3</hbase.version>
          <kafka.version>2.4.1</kafka.version>
          <lang3.version>3.9</lang3.version>
          <flink-connector-redis.verion>1.1.5</flink-connector-redis.verion>
      </properties>
      <dependencies>
          <dependency>
              <groupId>org.scala-lang</groupId>
              <artifactId>scala-reflect</artifactId>
              <version>${scala.version}.0</version>
          </dependency>
          <dependency>
              <groupId>org.scala-lang</groupId>
              <artifactId>scala-compiler</artifactId>
              <version>${scala.version}.0</version>
          </dependency>
          <dependency>
              <groupId>org.scala-lang</groupId>
              <artifactId>scala-library</artifactId>
              <version>${scala.version}.0</version>
          </dependency>
          <!-- kafka -->
          <dependency>
              <groupId>org.apache.kafka</groupId>
              <artifactId>kafka_${scala.version}</artifactId>
              <version>${kafka.version}</version>
          </dependency>
          <!-- flink 实时处理 -->
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-runtime-web_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-clients_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-streaming-scala_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-kafka_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-table-planner_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-json</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-csv</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-table-api-scala-bridge_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-redis_2.11</artifactId>
              <exclusions>
                  <exclusion>
                      <groupId>org.apache.flink</groupId>
                      <artifactId>flink-shaded-hadoop2</artifactId>
                  </exclusion>
                  <exclusion>
                      <groupId>org.apache.commons</groupId>
                      <artifactId>commons-lang3</artifactId>
                  </exclusion>
              </exclusions>
              <version>${flink-connector-redis.verion}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.commons</groupId>
              <artifactId>commons-lang3</artifactId>
              <version>${lang3.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-hive_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-hbase-2.2_${scala.version}</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <!-- mysql连接器 -->
          <dependency>
              <groupId>mysql</groupId>
              <artifactId>mysql-connector-java</artifactId>
              <version>${mysqlconnect.version}</version>
          </dependency>
          <!-- spark处理离线 -->
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-core_${scala.version}</artifactId>
              <exclusions>
                  <exclusion>
                      <groupId>org.apache.hive</groupId>
                      <artifactId>hive-exec</artifactId>
                  </exclusion>
              </exclusions>
              <version>${spark.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-sql_${scala.version}</artifactId>
              <exclusions>
                  <exclusion>
                      <groupId>org.apache.hive</groupId>
                      <artifactId>hive-exec</artifactId>
                  </exclusion>
              </exclusions>
              <version>${spark.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-hive_${scala.version}</artifactId>
              <version>${spark.version}</version>
          </dependency>
          <!-- hadoop相关 -->
          <dependency>
              <groupId>org.apache.hadoop</groupId>
              <artifactId>hadoop-client</artifactId>
              <version>${hdfs.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.hadoop</groupId>
              <artifactId>hadoop-auth</artifactId>
              <version>${hdfs.version}</version>
          </dependency>
          <!-- hbase 相关 -->
          <dependency>
              <groupId>org.apache.hbase</groupId>
              <artifactId>hbase-mapreduce</artifactId>
              <version>${hbase.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.hbase</groupId>
              <artifactId>hbase-client</artifactId>
              <version>${hbase.version}</version>
          </dependency>
          <!-- clickhouse -->
          <!--  连接ClickHouse需要驱动包 -->
          <dependency>
              <groupId>ru.yandex.clickhouse</groupId>
              <artifactId>clickhouse-jdbc</artifactId>
              <version>${clickhouse.version}</version>
              <!--  去除与Spark 冲突的包  -->
              <exclusions>
                  <exclusion>
                      <groupId>com.fasterxml.jackson.core</groupId>
                      <artifactId>jackson-databind</artifactId>
                  </exclusion>
                  <exclusion>
                      <groupId>net.jpountz.lz4</groupId>
                      <artifactId>lz4</artifactId>
                  </exclusion>
              </exclusions>
          </dependency>
          <!-- hudi依赖 -->
          <dependency>
              <groupId>org.apache.hudi</groupId>
              <artifactId>hudi-spark3-bundle_2.12</artifactId>
              <version>0.12.0</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flume</groupId>
              <artifactId>flume-ng-core</artifactId>
              <version>1.9.0</version>
              <!-- 因为部署的flume 有flume-ng-core这个包，所以使用provided过滤掉 -->
              <scope>provided</scope>
          </dependency>
      </dependencies>
      <build>
          <resources>
              <resource>
                  <directory>src/main/scala</directory>
              </resource>
              <resource>
                  <directory>src/main/java</directory>
              </resource>
              <resource>
                  <directory>src/main/resources</directory>
              </resource>
          </resources>
          <plugins>
              <plugin>
                  <groupId>net.alchim31.maven</groupId>
                  <artifactId>scala-maven-plugin</artifactId>
                  <version>3.2.2</version>
                  <configuration>
                      <recompileMode>incremental</recompileMode>
                  </configuration>
                  <executions>
                      <execution>
                          <goals>
                              <goal>compile</goal>
                              <goal>testCompile</goal>
                          </goals>
                      </execution>
                  </executions>
              </plugin>
              <plugin>
                  <groupId>org.apache.maven.plugins</groupId>
                  <artifactId>maven-compiler-plugin</artifactId>
                  <version>3.1</version>
                  <configuration>
                      <source>8</source>
                      <target>8</target>
                  </configuration>
              </plugin>
          </plugins>
      </build>
  ````

- #### **<a id="PackagingMethod">打包的方式</a>**

  - 方式一 ：右击项目--》Open Module Settings--》Artifacts--》+ JAR--》From modules with dependencies...-->ok-->只保留output(最后一个)--》ok--》Build--》Build Artifacts...--》Build--》out下面
  - 方式二：maven--》Lifecycle--》package--》右击--》Run Maven Build--》项目下面target目录下面--》展开--》2022_shtd_student-1.0-SNAPSHOT.jar

- #### <a id="sparksession">创建sparksession对象</a>

  ```
  // 创建sparksession对象
      val spark = SparkSession.builder().appName("IncrementalExtraction")
        .enableHiveSupport() // 开启hive支持
        .config("hive.exec.dynamic.partition", "true") // 开启动态分区
        .config("hive.exec.dynamic.partition.mode", "nonstrict") // 设置分区的模式是非严格模式
        .config("hive.exec.max.dynamic.partitions", 2000) // 设置分区的数量
        .config("spark.sql.parser.quotedRegexColumnNames", "true") // 允许在用引号引起来的列名称中使用正则表达式
        .getOrCreate()
  ```



<br>



## 三、[数据采集](#DataAcquisition)

### 1、<a id="IncrementalExtraction">增量抽取</a>

#### 1）数据流向

- 从MySQL中取出数据增量到hive中

#### 2）数据抽取

- **case1**、多表取modified_time最新（以modified_time做为增量字段）

  - 思路：从MySQL中拿出数据，在hive找到最大的modified_time值，追加模式写入hive的ods层，以etl_date为静态分区字段

    ```
    /*
        * todo CASE1  以modified_time做为增量字段
        *   将MySQL的 ds_db01 库中所有表的数据全量抽取到 Hive 的 ods 库中对应表中
        *   思路：从MySQL中拿出数据，在hive找到最大的modified_time值
        * */
    
        // 从MySQL中拿出数据
        val mysql_reader = spark.read.format("jdbc")
          .option("url", "jdbc:mysql://192.168.66.130:3306/ds_db01")
          .option("user", "root")
          .option("password", "123456")
    
        val etl_date = 20230828
    
        val tables = Array("brand_info", "coupon_info", "customer_addr", "customer_inf", "customer_level_inf", "customer_login", "favor_info", "order_cart", "order_detail", "order_master",
          "product_browse", "product_category", "product_comment", "product_info", "product_pic_info", "shipping_info", "supplier_info", "warehouse_info", "warehouse_product")
        tables.foreach(table => {
          //  到hive中去找最大的modified_time值
          val max_modified_time = spark.sql(s"select string(if(max(modified_time) is null,'',max(modified_time))) from 2023_ods1_ds_db01.${table}").first().getString(0)
          var sql = s"select * from ${table} "
          // 如果hive对应的表取出了最大日期，则MySQL查询时根据日期增量条件
          if (!max_modified_time.equals("")) {
            sql += s" where modified_time > '${max_modified_time}'"
          }
          // 此时df装的是从MySQL中取出来的数据
          val df = mysql_reader.option("dbtable", s"(${sql}) t1").load().withColumn("etl_date", lit(etl_date))
          // 追加模式写入hive，以etl_date为静态分区字段
          df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_ods1_ds_db01.${table}")
        })
    ```

    

- **case2**、以create_time为增量的操作customer_balance_log、customer_point_log表使用create_time作增量字段

  - 思路：如果hive对应的表取出了最大日期，则mysql查询时根据日期增量条件，追加模式写入hive，以etl_date为静态分区字段

    ````
    /*
        * todo CASE2  以create_time为增量的操作
        *   customer_balance_log、customer_point_log表使用create_time作增量字段
        * */
        val tables2 = Array("customer_balance_log", "customer_point_log")
        tables2.foreach(table => {
          val max_create_time = spark.sql(
            s"""
               |select string(if(max(create_time) is null,'',max(create_time)))
               |from 2023_ods1_ds_db01.${table}
               |""".stripMargin).first().getString(0)
          var sql = s"select * from ${table} "
          // 如果hive对应的表取出了最大日期，则mysql查询时根据日期增量条件
          if (!max_create_time.equals("")) {
            sql += s" where create_time > '${max_create_time}'"
          }
          // 此时df装的就是从mysql中取出对应的数据
          val df = mysql_reader.option("dbtable", s"(${sql}) t1").load().withColumn("etl_date", lit(etl_date))
          // 追加模式写入hive，以etl_date为静态分区字段
          df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_ods1_ds_db01.${table}")
        })
    ````

    

- **case3**、以login_time增量customer_login_log使用login_time作增量字段

  - 思路：如果hive对应的表取出了最大日期，则mysql查询时根据日期增量条件，追加模式写入hive，以etl_date为静态分区字段

    ```
    /*
        * todo CASE3  以login_time增量
        *   customer_login_log使用login_time作增量字段
        * */
        val tables3 = Array("customer_login_log")
        tables3.foreach(table => {
          val max_login_time = spark.sql(
            s"""
               |select string(if(max(login_time) is null,'',max(login_time)))
               |from 2023_ods1_ds_db01.${table}
               |""".stripMargin).first().getString(0)
          var sql = s"select * from ${table} "
          // 如果hive对应的表取出了最大日期，则mysql查询时根据日期增量条件
          if (!max_login_time.equals("")) {
            sql += s" where login_time > ${max_login_time}"
          }
          // 此时df装的就是从mysql中取出对应的数据
          val df = mysql_reader.option("dbtable", s"(${sql}) t1").load().withColumn("etl_date", lit(etl_date))
          // 追加模式写入hive，以etl_date为静态分区字段
          df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_ods1_ds_db01.${table}")
        })
    ```

- **CASE4**、coupon_use表取三个日期列最大值作为增量字段

  - 思路：消费券使用记录表以三列取最大的查询

    ````
    /**
         * todo CASE4
         * coupon_use表取三个日期列最大值作为增量字段
         * */
        // 消费券使用记录表以三列取最大的查询
        val max_time = spark.sql(
          """
            |select if(c is null,'',c)
            |from (select greatest(max(get_time),max(if(used_time='NULL','',used_time)),max(if(pay_time='NULL','',pay_time))) as c
            |      from 2023_ods1_ds_db01.coupon_use) as t1
            |""".stripMargin).first().getString(0)
        println(max_time)
        // 方法二：
        val coupon_df = mysql_reader.option("dbtable", "coupon_use").load()
          .where(array_max(array_remove(array("get_time", "used_time", "pay_time"), "NULL")).cast("string") > max_time)
          .withColumn("etl_date", lit(etl_date))
        println(coupon_df.count())
        coupon_df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_ods1_ds_db01.coupon_use")
    ````

    

### 2、<a id="FlumePort">Flume端口实时采集存入Kafka</a>

#### 1）数据从什么地方来

- 有一个脚本提供，作用（读取本地mysql中ds_db01数据库的数据，取其中一部分表发送给端口，取其中一部分表插入另外一个mysql数据库中ds_realtime_db）
  - 发送给端口的数据，由flume采集到kafka中
  - 插入到ds_realtime_db中的数据，由maxwell采集到kafka中

#### 2）脚本运行的前提（启动脚本应该在flume和Maxwell都配置好后）

- 脚本文件的颜色是绿色（chmod 777 jiaoben_socket）

- 本机mysql有ds_db01库和ds_realtime_db库（本机的主机名叫bigdata1）

  - 如果你的主机名不叫bigdata1，那么可以在/etc/hosts下强行增加一个映射：192.168.44.61 bigdata1

- 启动Flume、Kafka和Maxwell

  ```
  flume-ng agent -c conf -f /usr/local/src/flume/conf/jiaoben.conf --name a1 -DFlume.root.logger=INFO,console
  
  maxwell --config /usr/local/src/maxwell/config.properties -daemon
  
  
  三台机器分别启动kafka：
  kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
  ```

#### 3）启动Kafka

- ```
  三台机器分别启动kafka：
  kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
  ```

#### 4）创建主题

- ````
  kafka-topics.sh --create --bootstrap-server bigdata1:9092 --replication-factor 1 --partitions 1 --topic ods_mall_log
  ````

#### 5）启动消费者：(启动脚本后可用于观察数据)

- ```
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning
  ```

#### 6）查看主题

- ```
  kafka-topics.sh --list --bootstrap-server bigdata1:9092
  ```

#### 7）**flume启动**并且监听了25001**端口**

- ```
  vim jiaoben.conf
  
  #定义采集器的三个组件名称
  a1.sources=r1
  a1.sinks=k1
  a1.channels=c1
  
  #数据从哪来（某个机器的端口中来）
  a1.sources.r1.type=netcat
  a1.sources.r1.bind=localhost
  a1.sources.r1.port=25001
  
  #数据缓存到内存中
  a1.channels.c1.type=memory
  
  #数据打印到控制台（到哪去）
  a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
  a1.sinks.k1.brokerList=bigdata1:9092,bigdata2:9092,bigdata3:9092
  a1.sinks.k1.topic=ods_mall_log
  
  #绑定三者关联
  a1.sources.r1.channels=c1
  a1.sinks.k1.channel=c1
  ```

#### 8）启动flume

- ```
  flume-ng agent -c conf -f /usr/local/src/flume/conf/jiaoben.conf --name a1 -DFlume.root.logger=INFO,console
  ```

### 3、<a id="MaxwellGatherBinlog">Maxwell采集MySQL的binlog日志存入Kafka</a>

#### 1）使用Maxwell采集数据库ds_realtime_db中的变化，将变化的记录(json格式)发送给kafka：

- 修改mysql让其支持写binlog日志，并且指定日志类型以及数据库设置

  ```
  vi /etc/my.cnf
  
  在原来配置最后加入：
  #数据库id
  server-id=1
  #启动binlog，该参数的值会作为binlog的文件名
  log-bin=mysql-bin
  #binlog类型，maxwell要求为row类型
  binlog_format=row
  #启用binlog的数据库，需根据实际情况作出修改
  binlog-do-db=ds_realtime_db
  ```

- 修改maxwell的配置文件（连接mysql的登录配置、数据发送给kafka的配置）

  ```
  cp config.properties.example config.properties
  vi config.properties
          
  producer=kafka
  kafka.bootstrap.servers=bigdata1:9092,bigdata2:9092,bigdata3:9092
  kafka_topic=ods_mall_data
  
  # mysql login info
  host=localhost
  user=root
  password=123456
  ```

- 注意：

  - 如果重新配置maxwell后者监听其他新建的数据库，则需要将mysql中产生的maxwell数据库给删了

#### 2）启动Kafka

- ```
  三台机器分别启动kafka：
  kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
  ```

#### 3）创建主题

- ````
  kafka-topics.sh --create --bootstrap-server bigdata1:9092 --replication-factor 1 --partitions 1 --topic ods_mall_log
  ````

#### 4）启动消费者：(启动脚本后可用于观察数据)

- ```
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning
  ```

#### 5）查看主题

- ```
  kafka-topics.sh --list --bootstrap-server bigdata1:9092
  ```

#### 6）启动maxwell

- ````
  maxwell --config /usr/local/src/maxwell/config.properties -daemon
  ````

#### 7）启动脚本

- ```
  /opt/jiaoben.socket
  ```

#### 8）查看数据

- 查看全部数据

  ```
  # 查看Flume端口采集数据到Kafka的数据
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning
  
  # 查看Maxwell采集MySQL日志（binlog）数据到Kafka的数据
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_data --from-beginning
  ```

- 查看前2条数据

  ```
  kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic ods_mall_log --from-beginning --max-messages 2
  ```


#### 9）成功标准：两个消费者中都能看到数据

 







