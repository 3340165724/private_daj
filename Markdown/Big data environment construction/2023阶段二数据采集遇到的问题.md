# 2023阶段二数据采集遇到的问题

- ## 1、[分区模式](#nonstrict)

- ## 2、[写spark sql时字符串拼接](#StringConcatenation)

- ## 3、[字段名不存在](#UnknownColumn)

- ##  4、[不能强转](#cannot)

- ## 5、[按分区追加数据](#partition)



## 1、<a id="nonstrict">分区模式</a>

- ### 错误

  ````
  org.apache.spark.SparkException: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict
  ````

- ### 原因

  - 动态分区的模式默认是严格模式

- ### 解决

  ```
  在创建sparksession对象时添加
  .config("hive.exec.dynamic.partition.mode", "nonstrict") // 设置分区的模式是非严格模式
  
  // 在spark-shell中重新执行
  val spark = SparkSession.builder().appName("IncrementalExtraction")
        .enableHiveSupport() // 开启hive支持
        .config("hive.exec.dynamic.partition", "true") // 开启动态分区
        .config("hive.exec.dynamic.partition.mode", "nonstrict") // 设置分区的模式是非严格模式
        .config("hive.exec.max.dynamic.partitions", 2000) // 设置分区的数量
        .config("spark.sql.parser.quotedRegexColumnNames", "true") // 允许在用引号引起来的列名称中使用正则表达式
        .getOrCreate()
  ```




## 2、<a id="StringConcatenation">写spark sql时字符串拼接</a>

- ### 错误

  ```
  org.apache.spark.sql.catalyst.parser.ParseException:
  mismatched input '<EOF>' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 87)
  
  == SQL ==
  select string(if(max(create_time) is null,'',max(create_time))) from 2023_ods1_ds_db01.
  ---------------------------------------------------------------------------------------^^^
  ```

- ### 原因

  - 字符串拼接时前面少了个 **s**，导致后面用 **${}** 无法识别表名

- ### 解决

  ```
  val max_create_time = spark.sql(s"select string(if(max(create_time) is null,'',max(create_time))) from 2023_ods1_ds_db01.${table}").first().getString(0)
  ```

  

## 3、<a id="UnknownColumn">字段名不存在</a>

- ### 错误

  ```
  com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'max_create_time' in 'where clause'
  ```

- ### 原因

  - 1、**日期**类型的要用**引号包起来**
  - 2、sql 中 max_create_time字段是否是数据表中的字段

- ### 解决

  ```
  # max_create_time字段作为条件判断create_time > 'max_create_time'，日期类型要加引号
  
  // 如果hive对应的表取出最大时间，则MySQL查询根据最大时间作为增量条件
  var sql = s"select * from ${table}"
  if (!max_create_time.equals("")) {
      sql += " where create_time > 'max_create_time'"
      // 从MySQL中拿出数据,并添加一个字段符合hive的格式
      val df = mysql_reader.option("dbtable", s"(${sql}) as t1").load().withColumn("etl_date", lit(etl_date))
      df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable(s"2023_ods1_ds_db01.${table}")
  }
  
  ```




## 4、<a id="cannot">不能强转</a>

- ### 问题

  ```
  java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
  ```

- ### 原因

  - 写sql时没有把**星号（*）**换成对应的**最大值**，查出来的是dataframe通过 **.first().getString(0)**得到**第一行第一列的值**是一个Integer类型

- ### 解决

  ```
  val max_login_time = spark.sql(s"select string(if(max(login_time) is null,'',max(login_time))) from 2023_ods1_ds_db01.${table}").first().getString(0)
  ```




## 5、<a id="partition">按分区追加数据</a>

- ### 问题

  ```
  2023-09-06 20:13:16,041 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
  org.apache.spark.sql.AnalysisException: cannot resolve '`time`' given input columns: [spark_catalog.2023_ods1_ds_db01.coupon_use.coupon_id, spark_catalog.2023_ods1_ds_db01.coupon_use.coupon_status, spark_catalog.2023_ods1_ds_db01.coupon_use.coupon_use_id, spark_catalog.2023_ods1_ds_db01.coupon_use.customer_id, spark_catalog.2023_ods1_ds_db01.coupon_use.etl_date, spark_catalog.2023_ods1_ds_db01.coupon_use.get_time, spark_catalog.2023_ods1_ds_db01.coupon_use.order_sn, spark_catalog.2023_ods1_ds_db01.coupon_use.pay_time, spark_catalog.2023_ods1_ds_db01.coupon_use.used_time]; line 1 pos 89;
  'Project [unresolvedalias('if(isnull('c), , 'c), None)]
  +- 'SubqueryAlias t1
     +- 'Aggregate ['greatest(max(get_time#6), 'max('if((used_time#7 = NULL), , 'time)), max(if ((pay_time#8 = NULL))  else pay_time#8)) AS c#0]
        +- SubqueryAlias spark_catalog.2023_ods1_ds_db01.coupon_use
           +- HiveTableRelation [`2023_ods1_ds_db01`.`coupon_use`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [coupon_use_id#1, coupon_id#2, customer_id#3, order_sn#4, coupon_status#5, get_time#6, used_time#..., Partition Cols: [etl_date#9]]
  ```

- ### 原因

  - 追加模式写入hive的ods层，以etl_date为静态分区，少写了partitionBy("etl_date")

- ### 解决

  ```
  coupon_df.write.mode(SaveMode.Append).format("hive").partitionBy("etl_date").saveAsTable("2023_ods1_ds_db01.coupon_use")
  ```

  



















