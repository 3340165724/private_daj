

# 2023大数据环境搭建

## 一、[基本环境](#essential)

### 						1、[虚拟机IP](#ip)

### 						2、[防火墙设置](#firewalld)

### 						3、[修改主机名](#hostname)

### 						4、[映射](#hosts)

### 						5、[SSH免密](#ssh)

## 二、[配置环境变量](#path)

## 三、[组件](#bigdata)

### 				1、[Java](#java)

### 				2、[MySQL](#mysql)

### 				3、[Hadoop](#hadoop)

### 				4、[hive](#hive)

### 				5、[spark](#spark)

### 				6、[Flink](#flink)

### 				7、[Flume](#flume)

### 				8、[zookeeper](#zookeeper)

### 				9、[kafka](#kafka)

### 				10、[Hbase](#hbase)

### 11、[Clickhouse](#Clickhouse)

### 12、[maxwell](#maxwell)

### 13、[Redis](#Redis)

### 14、[Docker](#docker)

### 















## 一、<a id="essential">基本环境</a>

### 1、<a id="ip">先保证三台虚拟机的IP都修改正确，然后安装Xshell、Xftp工具</a>

- 1）、**编辑文件ifcfg-ens33**

  ```
  vi /etc/sysconfig/network-scripts/ifcfg-ens33
  
  # 修改配置文件
  BOOTPROTO=dhcp # 将dhcp（动态的）改为static（静态的）
  ONBOOT=no	# 将no改为yes
  # 在配置文件最下面添加
  IPADDR=92.168.125.128 # IP
  NETMASK=255.255.255.0  # 子网掩码
  GATEWAY=192.168.125.2  # 网关
  DNS1=114.114.114.114   # DNS
  # 保存并退出
  :wq
  ```

- 2、**重启网络服务**

  ```
  service network restart
  ```

- 3、**查看IP**

  ```
  ip addr
  ```



###  2、<a id="firewalld">防火墙设置</a>

- 1、**暂时关闭防火墙**

  ```
  systemctl stop firewalld
  ```

- 2、**永久关闭防火墙**

  ```
  systemctl disable firewalld
  ```

- 3、**查看防火墙状态**

  ```
  systemctl status firewalld
  
  注意：#提示 Active: inactive (dead)  则关闭成功
  ```

  

## 3、<a id="hostname">修改三台虚拟机的主机名（两个结合起来用）</a>

```
vi /etc/hostname
主机名

hostnamectl set-hostname 主机名


【输入hostname可以查看当前机器的主机名】
```



## 4、<a id="hosts">三台机器主机名和IP进行映射，后续通过主机名就可以访问对应的机器</a>

```
 vi /etc/hosts
 
          加入映射内容如下：
xxx.xxx.xx.xxx   bigdata1
xxx.xxx.xx.xxx   bigdata2
xxx.xxx.xx.xxx   bigdata3
  IP			   主机名
```

- 远程拷贝文件覆盖到其他机器

  ```
  scp 本机路径 bigdata2：bigdata2机器的路径
  ```





## 5、<a id="ssh">配置SSH免密登录</a>

- 普通情况【未配置免密登录时】：ssh root@192.168.44.XXX 会提示你输入yes、输入密码比较麻

- 所以需要配置ssh免密登录到其他机器

- 1、在第一台机器master机器上产生秘钥

  ```
  ssh-keygen -t rsa
  ```

- 2、将秘钥对应数据拷贝到三台机器（包括自己）

  ```
  ssh-copy-id -i root@主机名1
  ssh-copy-id -i root@主机名2
  ssh-copy-id -i root@主机名3
  ```
  
- 测试

  ```
  在master主机上输入： ssh 其他主机名
  ```

  

## 二、<a id="path">配置环境变量</a>

- 针对root账户生效的环境变量文件：/root/.bash_profile
- 针对所有账户生效的环境变量文件：/etc/profile

### 1、先获取路径

```
pwd
```

### 2、复制路径

### 3、进入到**/root/.bash_profile**文件编辑

```
vim /root/.bash_profile
```

### 4、在/root/.bash_profile文件中输入

```
# JAVA_HOME
export JAVA_HOME=/usr/local/src/jdk
export PATH=$PATH:$JAVA_HOME/bin

# HADOOP_HOME
export HADOOP_HOME=/usr/local/src/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin


# HIVE_HOME
export HIVE_HOME=/usr/local/src/hive
export PATH=$HIVE_HOME/bin

# SPARK_HOME 
export SPARK_HOME=/usr/local/src/spark
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin

# FLINK_HOME
export FLINK_HOME=/usr/local/src/flink
export PATH=$PATH:$FLINK_HOME/bin

# FLUME_HOME
export FLUME_HOME=/usr/local/src/flume
export PATH=$PATH:$FLUME_HOME/bin

# ZOOKEEPER_HOME
export ZOOKEEPER_HOME=/usr/local/src/zookeeper
export PATH=$PATH:$ZOOKEEPER/bin

# KAFKA_HOME
export KAFKA_HOME=/usr/local/src/kafka
export PATH=$PATH:$KAFKA_HOME/bin

# HBASE_HOME
export HBASE_HOME=/usr/local/src/hbase
export PATH=$PATH:$HBASE_HOME/bin

# MAXWELL_HOME
export MAXWELL_HOME=/usr/local/src/maxwell
export PATH=$PATH:$MAXWELL_HOME/bin

//保存并退出
：wq
```

### 5、让修改后的文件生效

```
source /root/.bash_profile
```





##  三、<a id="bigdata">组件</a>

### 1、<a id="java">Java</a>

#### 	1)、确认当前虚拟机中是否自带安装jdk	

```
java -version
rpm -qa | grep java
```

- 如果出现了内容，则进行如下命令卸载

  ```
  rpm -e --nodeps 
  # --nodeps后面的内容则是rpm -qa 查询出来的内容复制过来
  ```

#### 2)、解压jdk压缩文件到指定目录

```
tar -zxvf jdk...tar.gz -C /usr/local/src
```

####  3)、进入/usr/local/src目录，对解压后的jdk目录重命名【改短一点】

```
 mv jdk1.8.0_212/   jdk
```

####  4)、[配置Jdk环境变量【linux能够很好的识别到Java相关的命令】](#path)

```
export JAVA_HOME=/usr/local/src/jdk
export PATH=$PATH:$JAVA_HOME/bin
```

####  5)、然后环境变量马上生效

```
 source  /root/.bash_profile
```

####  6)、测试

```
java -version 查看jdk版本
java或者javac
```

####  7)、将jdk远程拷贝至其他两台机器对应的目录下

- 将环境变量配置文件也同步拷贝到其他两台机器对应目录下

  ```
  scp -r /usr/local/src/jdk  bigdata2:/usr/local/src
  scp /root/.bash_profile bigdata2:/root/
  ```

- 注意：其他两台机器也需要source一下，否则环境变量不生效，输入命令无法识别



### 2、<a id="mysql">MySQL</a>

#### 1)、因centos7中自带了mariadb数据库，会和mysql发生冲突，先将其卸载

- 查询出所有的内容

  ```
  rpm -qa | grep mariadb
  ```

- 卸载

  ```
  rpm -e --nodeps
  ```

#### 2)、创建MySQL用户组和用户

```
groupadd mysql
useradd -r -g mysql mysql
```

- 如果创建用户组出错

  ```
  bash: groupadd: command not found
  
  解决：
  su -
  ```

#### 3)、解压MySQL安装包到指定位置

```
tar -zxvf /mysoft/mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz -C /usr/local/src/
```

#### 4)、进入/usr/local/src下，将解压后的目录改个名字

```
mv mysql-5.7.40-linux-glibc2.12-x86_64/ mysql5.7
```

#### 5)、修改mysql5.7目录及子目录权限

```
chown -R mysql:mysql /usr/local/src/mysql5.7
chmod -R 755 /usr/local/src/mysql5.7/
```

#### 6)、进入/usr/local/src/mysql5.7/bin/下，执行初始化mysql命令

```
./mysqld --initialize --user=mysql --datadir=/usr/local/src/mysql5.7/data --basedir=/usr/local/src/mysql5.7
```

- 【注意：将初始化最后生成的mysql第一次默认登录密码复制出来】

#### 7)、新建编辑/etc/my.cnf，并授权

- 新建

  ```
  [mysqld]
  datadir=/usr/local/src/mysql5.7/data
  port=3306
  sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
  symbolic-links=0
  max_connections=400
  innodb_file_per_table=1
  #表名大小写不明感，敏感为
  lower_case_table_names=1
  ```

- 授权

  ```
  chmod -R 775 /etc/my.cnf
  ```

#### 8)、修改/usr/local/src/mysql5.7/support-files/目录下的mysql.server文件

```
if test -z "$basedir"
then
  basedir=/usr/local/src/mysql5.7      （1个）
  bindir=/usr/local/src/mysql5.7/bin   （2个）
  if test -z "$datadir"
  then
    datadir=/usr/local/src/mysql5.7/data（3个）
  fi
  sbindir=/usr/local/src/mysql5.7/bin（4个）
  libexecdir=/usr/local/src/mysql5.7/bin（5个）
```

#### 9)、启动服务

- 进入到/usr/local/src/support-files/目录下执行

  ```
   /usr/local/src/mysql5.7/support-files/mysql.server start
   
   【不出意外会显示SUCCESS成功】
  ```

#### 10)、创建软连接方便后续启动mysql

```
ln -s /usr/local/src/mysql5.7/support-files/mysql.server /etc/init.d/mysql
ln -s /usr/local/src/mysql5.7/bin/mysql /usr/bin/mysql
```

#### 11)、测试（重新启动mysql）

```
service mysql restart
```

#### 12)、登录mysql修改初始密码

```
mysql -u root -p输入前面初始化后的第一次默认密码  
```

- 初始化密码无效

- 解决方案

  - 跳过验证登录

    ```
    在/etc/my.cnf配置中加skip-grant-tables
    
    vim /etc/my.cnf
    
    skip-grant-tables
    :wq
    ```

  - 登录MySQL

    ```
    mysql
    ```

  - 设置密码

    ```
    update mysql.user set authentication_string=password('123456') where user='root';
    ```

#### 13)、修改密码

```
set password for root@localhost = password('123456');
```

#### 14）、开启远程登录（否则其他机器无法连接上该mysql）

````
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON *.* TO 'root'@'192.168.xx.xx' IDENTIFIED BY '123456' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' IDENTIFIED BY '123456' WITH GRANT OPTION;
flush privileges;
````

#### 14)、如果设置了跳过验证登录，需要进到配置文件注释







###  3、<a id="hadoop">Hadoop</a>

#### 1)、解压

```
tar -zxvf hadoop-3.1.3.tar.gz -C /usr/local/src/
```

#### 2)、进入/usr/local/src下，对解压的目录进行重命名

```
cd /usr/local/src
mv hadoop-3.1.3/ hadoop
```

#### 3）、[配置环境变量](#path)

```
vim /root/.bash_profile

加入：
export HADOOP_HOME=/usr/local/src/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

#### 4)、查看Hadoop是否配置成功

```
hadoop version
```

#### 5)、进入到/usr/local/src/hadoop/etc/hadoop/中配置文件的目录，修改5个配置文件

- hadoop-env.sh（Hadoop3.x都要加，否则启动时会报错）

  ```
  export JAVA_HOME=/usr/local/src/jdk
  
  export HDFS_NAMENODE_USER=root
  export HDFS_DATANODE_USER=root
  export HDFS_SECONDARYNAMENODE_USER=root
  export YARN_RESOURCEMANAGER_USER=root
  export YARN_NODEMANAGER_USER=root
  ```

- core-site.xml

  ```
  <!-- 指定HDFS中NameNode的地址 -->
  <!-- 在java代码中配置hadoop需要调用 -->
  <property>
  	<name>fs.defaultFS</name>
  	<value>hdfs://bigdata1:9000</value>
  </property>
  
  <!-- 运行Hadoop时产生的临时文件存储的目录 -->
  <property>
  	<name>hadoop.tmp.dir</name>
  	<value>/opt/data/hadoop</value>
  </property>
  ```

- hdfs-site.xml

  ```
  <!-- 副本个数 -->
  <property>
  	<name>dfs.replication</name>
  	<value>3</value>
  </property>
  ```

- yarn-site.xml

  ```
  <!-- 配置yarn主节点的位置(指定YARN的ResourceManager的地址) -->
  <property>
  	<name>yarn.resouremanager.hostname</name>
  	<value>bigdata2</value>
  </property>
  
  <property>
  	<name>yarn.nodemanager.aux-services</name>
  	<value>mapreduce_shuffle</value>
  </property>
  
   <!--关闭虚拟内存验证-->
  <property>
  	<name>yarn.nodemanager.vmem-check-enabled</name>
  	<value>false</value>
  </property>
  
  <!--关闭物理内存验证-->
  <property>
  	<name>yarn.nodemanager.pmem-check-enabled</name>
  	<value>false</value>
  </property>
  ```

- mapred-site.xml

  ```
  <!-- 指定MR运行在YARN上 -->
  <property>
  	<name>mapreduce.framework.name</name>
  	<value>yarn</value>
  </property>
  ```

- workers

  ```
  bigdata1
  bigdata2
  bigdata3
  ```

#### 6)、将配置好的hadoop远程分发给另外两台机器

```
scp -r /usr/local/src/hadoop bigdata2:/usr/local/src
scp -r /usr/local/src/hadoop bigdata3:/usr/local/src

scp /root/.bash_profile bigdata2:/root/
scp /root/.bash_profile bigdata3:/root/

在bigdata2机器上：source /root/.bash_profile
在bigdata3机器上：source /root/.bash_profile
```

#### 7)、格式化namenode

```
hdfs namenode -format
```

- 如果需要重置hadoop文件系统重新格式化的话，那么将core-site.xml中配置的那个hadoop.tmp.dir目录给删了，让其重新创建

#### 8)、启动hadoop集群（在主节点上）

- 一键启动所有

  ```
  start-all.sh
  ```

- 分开启动

  ```
  启动hdfs：  start-dfs.sh
  启动yarn：  start-yarn.sh
  ```

#### 9)、检测是否成功

- 在主节点上输入 jps

  ```
  namenode
  datanode
  nodemanager
  resourcemanager
  secondarynamendoe
  ```

- 在其他从机节点上输入jps

  ```
  datanode
  nodemanager
  ```

- 查看是否可以打开hadoop的webUI管理界面

  ```
  主节点虚拟机IP:9870
  ```

#### 10)、运行案例

- 运行

  ```
  hadoop jar /usr/local/src/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /test/word.txt /out
  ```

  


### 4、<a id="hive">hive</a>

#### 1)、解压hive安装包到指定目录

```
tar -zxvf /mysoft/apache-hive-3.1.2-bin.tar.gz -C /usr/local/src/
```

#### 2)、重命名

```
mv apache-hive-3.1.2-bin/ hive
```

#### 3)、 [环境变量](#path)

```
vim /root/.bash_profile

export HIVE_HOME=/usr/local/src/hive
export PATH=$PATH:$HIVE_HOME/bin
```

#### 4)、修改配置文件 hive-site.xml

- 1、复制hive-default.xml.template一个进行修改

  - 改动五处(数据库链接地址,驱动 , 用户名 , 密码,版本效验)

  ```
   <!-- jdbc 连接的 URL -->
   <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://192.168.66.130:3306/myhive1?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>
  
    <!-- jdbc 连接的 Driver-->
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
    </property>
  
    <!-- jdbc 连接的 username-->
    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
      <description>Username to use against metastore database</description>
    </property>
  
    <!-- jdbc 连接的 password -->
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>123456</value>
    </property>
  
   <!-- Hive 元数据存储版本的验证 -->
   <property>
      <name>hive.metastore.schema.verification</name>
      <value>false</value>
    </property>
  ```

  - 删除三处（启动hive时报system.io...错误，则删除配置项目：）

    ```
     <property>
        <name>hive.querylog.location</name>
        <value></value>
        <description>Location of Hive run time structured log file</description>
      </property>
    
      <property>
        <name>hive.downloaded.resources.dir</name>
        <value></value>
        <description>Temporary local directory for added resources in the remote file system.</description>
      </property>
    
      <property>
        <name>hive.exec.local.scratchdir</name>
        <value></value>
        <description>Local scratch space for Hive jobs</description>
      </property>
    ```

- 2、自己创建一个空白的hive-site.xml加入配置【推荐】

  - 去复制头文件，在添加配置
  
  ```
   <!-- jdbc 连接的 URL -->
   <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://192.168.66.130:3306/myhive1?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>
  
    <!-- jdbc 连接的 Driver-->
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
    </property>
  
    <!-- jdbc 连接的 username-->
    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
      <description>Username to use against metastore database</description>
    </property>
  
    <!-- jdbc 连接的 password -->
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>123456</value>
    </property>
  
   <!-- Hive 元数据存储版本的验证 -->
   <property>
      <name>hive.metastore.schema.verification</name>
      <value>false</value>
    </property>
  ```



#### 5)、将java连接mysql的驱动包拷贝到hive下的lib下

```
cp /opt/soft/mysql-connector-java-5.1.37.jar /usr/local/src/hive/lib/
```

#### 6)、冲突包的解决（hive3.1.2这个版本里面的包和hadoop的包产生了冲突，需要处理一下）

- 将log4j-slf4j的一个jar包改个名字

  ```
  mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
  ```

- 删除hive/lib下的guava-19.0.jar【必须修改的】

  ```
   rm -rf /usr/local/src/hive/lib/guava-19.0.jar
  ```

-  将hadoop下的guava包复制到hive的lib下：

  ```
  cp /usr/local/src/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/src/hive/lib/
  ```

#### 7)、初始化hive元数据仓库【在hive元数据存放的mysql中创建一系列元数据表】

```
schematool -dbType mysql -initSchema 
```

- 出现-bash: schematool: command not found（未找到命令），只需要让环境变量生效就行（source /root/.bash_profile）

####  8)、hive命令

- hive命令

  - 查看所有的数据库

    ```
    show databases;
    ```

  - 创建数据库

    ```
    create database 数据库名; 
    ```

  - 使用某个数据库

    ```
    use 数据库名 ;
    ```

  - 查看某个数据库下的所有表

    ```
    show tables;
    ```

- 创建表

  ```
  create [external] table 表名(列名 类型,列名 类型,列名 类型)
  row format delimited 
  fields terminated by '\t' 
  lines terminated by '\n';
  【fields terminated by 代表文件中每行数据字段分隔符】
  【lines terminated by 代表文件中行与行之间的分隔符】
  
  
  create table tb_student(
  sid int,sname string, sex int, birthday date, phone string, address string,scid int,reg_data date)
  row format delimited
  fields terminated by ','
  lines terminated by '\n';
  
  ```

- 数据导入表中

  -  1》将**hdfs中**的数据文件导入hive中

    ```
    load data inpath '/test1/tb_student.txt' overwrite into table tb_student;
    					hdfs路径				完全覆盖					表名
    ```

  - 2》将**虚拟机中的数据**文件导入hive中

    ```
     load data local inpath '/opt/test/tb_student.txt' overwrite into table tb_student;
     							Linux路径
    ```

  -  3》清除表中的数据

    ```
    truncate table 表名;  【不能删除表中的分区，只能删除表中的数据】
    ```

- 数据导出

  - 查询的结果导出**到本地**
  
    ```
    insert overwrite local directory '/home/hadoop/data/major_student_counts.csv'
    row format delimited
    fields terminated by ','
    select  *  from student;
    ```

  - 查询的结果导出**到HDFS**

    ```
    insert overwrite local directory '/home/hadoop/data/major_student_counts.csv'
    row format delimited
    fields terminated by ','
    select  *  from student;
    ```
  
    
  
- 创建分区表

  ```
  create [external] table 表名(列名 类型,列名 类型,列名 类型)
  partitioned by(分区列名 类型)
  row format delimited 
  fields terminated by '\t' 
  lines terminated by '\n';
  【partitioned by 即是创建表的分区】
  ```

  - 静态分区

    ```
    load data local inpath '/opt/test/tb_student.txt' overwrite into table tb_student partition(分区列名=值);  【分区列为字符串的话，='值'】
    ```

  - 【为什么要分区：分区在hive中创建后，在hdfs中体现的就是一个目录，目的是按分区列将数据切分为小文件，在检索的时候如果使用到了分区列，则只会涉及到该分区下的文件数据，提升检索使用的效率】

  -  查看表的分区信息

    ```
    show partitions 表名;
    show partitions 库名.表名;
    ```

  - 删除分区

    ```
    alter table 表名 drop partition(分区列=分区列的值)
    alter table 表名 drop partition(分区列 <= 2022)
           【删除分区后该分区的数据也会被同步删除】
    ```

- 查看表结构命令

  - 查看表的列信息

    ```
    desc 表名;
    ```

  - 查看更加详细的表信息

    ```
    desc extended tb_class; 
    ```

  - 查看建表语句

    ```
    show create table tb_class;
    ```

  - 查看hive中所有的函数

    ```
    show functions;
    ```






### 5、<a id="spark">spark</a>

#### 1)、解压

```
tar -zxvf spark-3.1.1-bin-hadoop3.2.tgz -C /usr/local/src/
```

#### 2)、重命名（看要求）

```
mv spark-3.1.1 spark
```

#### 3)、[配置环境变量](#path)

```
# SPARK_HOME
export SPARK_HOME=/usr/local/src/spark
export PATH=$PATH:SPARK_HOME/bin
export PATH=$PATH:SPARK_HOME/sbin
```

#### 4)、配置文件

- 复制spark-env.sh.template 为spark-env.sh

  ```
  cp spark-env.sh.template  spark-env.sh
  
  # 配置文件中可以找到
  export JAVA_HOME=/usr/local/src/jdk
  export HADOOP_CONF_DIR=/usr/local/src/hadoop/etc/hadoop
  
  export SPARK_MASTER_HOST=bigdata1 
  						主节点的主机名
  export SPARK_MASTER_PORT=7077
  
  export SPARK_CONF_DIR=/usr/local/src/spark/conf
  ```

- 修改worker工作任务节点（ 将三台机器主机名放入）

  ```
  cp workers.template workers
  
  bigdata1
  bigdata2
  bigdata3
  ```

#### 5)、远程拷贝spark到其他两台机器上

```
scp -r /usr/local/src/spark bigdata2:/usr/local/src/
scp -r /usr/local/src/spark bigdata3:/usr/local/src/
```

#### 6)、启动集群

```
sbin/start-all.sh
```

#### 7)、查看是否配置成功

- 查看进程

  ```
  # bigdata1
  3075 Jps
  3012 Worker
  2917 Master
  
  
  # bigdata2
  7334 Jps
  7256 Worker
  
  
  # bigdata1
  7334 Jps
  7351 Worker
  ```

- 访问web界面

  ```
  192.168.66.130:8080
   Master节点     端口
  ```

#### 8)、运行自带的Pi值计算案例

- spark on yarn方式来运行 （无需启动spark集群，但是要启动hadoop）

  ```
  spark-submit --master yarn --class org.apache.spark.examples.SparkPi --executor-memory 1g --total-executor-cores 2  /usr/local/src/spark/examples/jars/spark-examples_2.xxxx......jar  100
  ```

- spark集群运行（无需启动hadooop，但是要启动spark）

  ```
  spark-submit --master spark://bigdata1:7077 --class org.apache.spark.examples.SparkPi --executor-memory 1g --total-executor-cores 2  /usr/local/src/spark/examples/jars/spark-examples_2.xxxx......jar  100
  ```

  - 如果使用spark-on-yarn运行时，需要在hadoop的yarn-site.xml中加入关闭内存检测配置，否则报错

    ```
    <property>
         <name>yarn.nodemanager.pmem-check-enabled</name>
         <value>false</value>
    </property>
    <property>
         <name>yarn.nodemanager.vmem-check-enabled</name>
         <value>false</value>
    </property>
    ```


#### 9）、spark-sql + hive（用spark直接操作hive）

- 将hive-site.xml文件拷贝到spark/conf下

  ````
  cp /usr/local/src/hive/conf/hive-site.xml /usr/local/src/spark/conf
  ````

- 将MySQL驱动拷贝到spark/jars下

  ```
  scp /opt/soft/mysql-connector-java-5.1.37.jar bigdata2:/usr/local/src/spark/jars/
  ```

  



### 6、<a id="flink">Flink</a>

#### 1)、解压

```
tar -zxvf flink-1.14.0-bin-scala_2.12.tgz /usr/local/src/
```

#### 2)、重命名（看要求）

```
mv flink-1.14.0 flink
```

#### 3)、[配置环境变量](#path)

```
# FLINK_HOME
export FLINK_HOME=/usr/local/src/flink
export PATH=$PATH:$FLINK_HOME/bin
```

#### 4)、修改flink配置文件

- 进入flink下conf目录，修改flink-conf.yaml配置

  ```
  jobmanager.rpc.address: bigdata1   //(注意冒号后有空格)
  					   主节点的主机名
  					   
  // 关闭类加载器泄漏检查
  classloader.check-leaked-classloader: false
  ```

- 修改workers，将几个机器的节点名都写入

  ```
  bigdata1
  bigdata2
  bigdata3
  ```

#### 5)、分发

```
scp -r /usr/local/src/flink slave1:/usr/local/src/
scp -r /usr/local/src/flink slave2:/usr/local/src/
```

#### 6)、启动flink集群

```
start-cluster.sh

 停止flink： stop-cluster.sh
```

#### 7)、查看是否配置成功

- 查看进程

  ```
  # bigdata1
  3553 StandaloneSessionClusterEntrypoint
  3917 TaskManagerRunner
  3999 Jps
  
  
  # bigdata2
  7334 Jps
  7687 TaskManagerRunner
  
  
  # bigdata3
  7334 Jps
  7687 TaskManagerRunner
  ```

- WebUI界面访问查看

  ```
  192.168.66.130:8081
       ip       web访问端口
  ```

#### 8)、运行方式

- flink on yarn（flink集群无需启动，但是要启动hadoop）

  - 批处理离线文件 （flink/examples/batch/WordCount.jar）

    ```
    flink run -m yarn-cluster  /usr/local/src/flink/examples/batch/WordCount.jar
    ```

    - 直接运行会报错

      ```
      ------------------------------------------------------------
       The program finished with the following exception:
      
      java.lang.IllegalStateException: No Executor found. Please make sure to export the HADOOP_CLASSPATH environment variable or have hadoop in your classpath. For more information refer to the "Deployment" section of the official Apache Flink documentation.
      ```

    - 解决

      ```
      export HADOOP_CLASSPATH=`${HADOOP_HOME}/bin/hadoop classpath` flink run -m yarn-cluster /usr/local/src/flink/examples/batch/WordCount.jar
      ```
    
  - 流失处理 （flink/examples/streaming/SocketWindowWodCount.jar）
  
    - 1、先监听端口nc -l -p 端口号(可随意）
  
    - 2、flink run -m  yarn-cluster sockeWindoWoreCount.jar  --hostname bigdata1(主机名) --port  跟监听端口对应
  
      ```
      新打开一个终端：nc -l 12000
      需要配置：export HADOOP_CLASSPATH=`${HADOOP_HOME}/bin/hadoop classpath`
      
      flink run -m  yarn-cluster  /usr/local/src/flink/examples/streaming/sockeWindoWoreCount.jar  --hostname bigdata1 --port 12000
      ```
  
    - 注意：要开两个窗体一个用来设置监听端口，另一个用来实时监听![flink](C:\Users\Lenov\Pictures\MarkDown\flink.png)
  
- flink集群运行（flink集群需启动，hadoop不用启动）

  - 批处理离线文件 （flink/examples/batch/WordCount.jar）

    ```
    flink run /usr/local/src/flink/examples/batch/WordCount.jar
    ```

  - 流失处理 （flink/examples/streaming/SocketWindowWodCount.jar）

    - 1、先监听端口nc -l -p 端口号(可随意）

    - 2、flink run -m  yarn-cluster sockeWindoWoreCount.jar  --hostname bigdata1(主机名) --port  跟监听端口对应

      ```
      新打开一个终端：nc -l 12000
      需要配置：export HADOOP_CLASSPATH=`${HADOOP_HOME}/bin/hadoop classpath`
      flink run /usr/local/src/flink/examples/streaming/sockeWindoWoreCount.jar  --hostname bigdata1 --port 12000
      ```

    - 注意：要开两个窗体一个用来设置监听端口，另一个用来实时监听![flink](C:\Users\Lenov\Pictures\MarkDown\flink.png)


### 7、<a id="flume">Flume</a>

#### 1)、解压

```
tar -zxvfapache-flume-1.9.0-bin.tar.gz /usr/local/src/
```

#### 2)、重命名（看要求）

```
mv apache-flume-1.9.0 flume
```

#### 3)、[配置环境变量](#path)

```
export FLUME_HOME=/usr/local/src/flume
export PATH=$PATH:$FLUME_HOME/bin
```

#### 4)、查看flume环境变量是否生效

```
flu + tab键 =====》 flume -ng 
```

#### 5)、配置采集器的内容

- 1）**采集端口的数据**并打印到控制台

  ```
  vim duankou.conf
  
  #定义采集器的三个组件名称
  a1.sources=r1
  a1.channels=c1
  a1.sinks=k1
  
  #数据从哪来（某个机器的端口中来）
  a1.sources.r1.type=netcat
  a1.sources.r1.bind=bigdata1
  a1.sources.r1.port=12000
  
  #数据缓存到内存中
  a1.channels.c1.type=memory
  
  #数据打印到控制台（到哪去）
  a1.sinks.k1.type=logger
  
  #绑定三者关联
  a1.sources.r1.channels=c1
  a1.sinks.k1.channel=c1
  ```

  - 监听端口

    - 在其他机器上执行（先启动flume对端口进行监听，然后使用nc连接这个端口发送数据）

    ```
    nc bigdata1 12000
    ```

- 2）**采集文件中**内容变化的数据

  ```
  # 创建并打开文件
  vim file.conf
  
  #定义采集器的三个组件名称
  a1.sources=r1
  a1.channels=c1
  a1.sinks=k1
  
  #数据从哪来（某个机器的端口中来）
  a1.sources.r1.type=exec
  a1.sources.r1.command=tail -F /opt/file/work.txt
  
  #数据缓存到内存中
  a1.channels.c1.type=memory
  
  #数据打印到控制台（到哪去）
  a1.sinks.k1.type=logger
  
  #绑定三者关联
  a1.sources.r1.channels=c1
  a1.sinks.k1.channel=c1
  ```

  - 监听文件（向文件中追加内容）

    ```
    echo "内容">>/opt/file/work.txt
    ```

- Flume采集数据后存入Kafka中

  ```
  vim flume_kafka.conf
  
  #定义采集器的三个组件名称
  a1.sources=r1
  a1.channels=c1
  a1.sinks=k1
  
  
  #数据从哪来（某个机器的端口中来）
  a1.sources.r1.type=netcat
  a1.sources.r1.bind=bigdata1
  a1.sources.r1.port=12000
  
  
  #数据缓存到内存中
  a1.channels.c1.type=memory
  
  
  #数据放到kafka中（到哪去）
  a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
  a1.sinks.k1.brokerList=bigdata1:9092,bigdata2:9092,bigdata3:9092
  a1.sinks.k1.topic=lol（主题名）
  a1.sinks.k1.serializer.class=kafka.serializer.StringEncoder
  
  	【不同版本配置单词写法略有差异】
  	a1.sinks.k1.kafka.topic = 主题名
  	a1.sinks.k1.kafka.bootstrap.servers = bigdata1:9092,bigdata2:9092,bigdata3:9092
  
  
  #绑定三者关联
  a1.sources.r1.channels=c1
  a1.sinks.k1.channel=c1
  ```

- 双Sink（分别sink到控制台、kafka中）

  ```
  #定义采集器的三个组件名称
  a1.sources=r1
  a1.channels=c1 c2
  a1.sinks=k1 k2
  
  #数据从哪来（某个机器的端口中来）
  a1.sources.r1.type=netcat
  a1.sources.r1.bind=bigdata1
  a1.sources.r1.port=12000
  
  #数据缓存到内存中
  a1.channels.c1.type=memory
  a1.channels.c2.type=memory
  
  #数据打印到控制台（到哪去）
  a1.sinks.k2.type=logger
  #数据放到kafka中（到哪去）
  a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
  a1.sinks.k1.brokerList=bigdata1:9092,bigdata2:9092,bigdata3:9092
  a1.sinks.k1.topic=lol（主题名）
  a1.sinks.k1.serializer.class=kafka.serializer.StringEncoder
  
  #绑定三者关联
  a1.sources.r1.channels=c1 c2
  a1.sinks.k1.channel=c1
  a1.sinks.k2.channel=c2
  ```

#### 6)、启动flume

```
flume-ng agent -c conf -f /usr/local/src/flume/conf/duankou.conf --name a1 -Dflume.root.logger=INFO,console
								采集创建的文件路径 		文件名		等于: -n 		
```

#### 7)、测试

- 1）测试端口的话：使用其他机器终端，nc 主机名 端口号

  ```
  在 bigdata2上
  nc bigdata1 12000
  ```

- 2）测试文件的话：  echo "内容">>文件名

#### 8)、了解

- 同时将采集的数据放入kafka和hdfs中 (sink到什么地方)
- https://blog.csdn.net/qq_39604679/article/details/123669831



### 8、<a id="zookeeper">zookeeper</a>

#### 1)、解压

```)
tar -zxvf zookeeper-3.5.7.tar.gz /usr/local/src/
```

#### 2)、重命名（看要求）

```
mv zookeeper-3.5.7 zookeeper
```

#### 3)、[配置环境变量](#path)

```
export ZOOKEEPER_HOME=/usr/local/src/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```

#### 4)、配置文件

- 进入zookeeper/conf下，复制zoo_sample.cfg配置文件命令为 zoo.cfg

  ```
  cp zoo_sample.cfg zoo.cfg
  vim zoo.cfg
  
  
  dataDir=/usr/local/src/zookeeper/zkdata
  dataLogDir=/usr/local/src/zookeeper/zklogs
  
  server.1=bigdata1:2888:3888
  server.2=bigdata2:2888:3888
  server.3=bigdata3:2888:3888
  ```

- 在dataDir目录下新建一个myid的文件，里面写上1

  ```
  cd /usr/local/src/zookeeper
  
  mkdir zkdata
  mkdir zklogs
  
  vim /usr/local/src/zookeeper/zkdata/myid
  1
  ```

#### 5)、将zookeeper拷贝至其他机器

```
scp -r /usr/local/src/zookeeper/ bigdata2:/usr/local/src/
scp -r /usr/local/src/zookeeper/ bigdata3:/usr/local/src/
```

#### 6)、分别在bigdata2和bigdata3中zookeeper的dataDir下修改myid分别为2、3

#### 7)、分别三台机器上启动zookeeper

```
zkServer.sh start
```

- 启动时报错already running as process【因为机器异常关闭缓存目录中残留PID文件导致的(为关闭进程强行关机等导致的）】

  ```
  # 进入文件目录dataDir目录，删除zookeeper_server.pid 和 version-2，重新启动
  
  cd /usr/local/src/zookeeper/zkdata
  
  rm -rf zookeeper_server.pid
  rm -rf version-2
  ```


#### 8)、查看状态

```
zkServer.sh status
```

- 一直报错
  - Client port found: 2181. Client address: localhost.
  - Error contacting service. It is probably not running.
- 解决
  - 1、检查三台机器的防火墙是否关闭，没关把防火墙关了
  - 2、三台机器上分别关闭服务，然后，先在bigdata2上再次启动服务，在启动bigdata1、bigdata3

#### 9)、进程

```
zookeeper启动后的进程：QuorumPeerMain
```



### 9、<a id="kafka">Kafka</a>

#### 1、解压

```
tar -zxvf kafka_2.12-2.4.1.tgz /usr/local/src/
```

#### 2、重命名（看要求）

```
mv kafka_2.12-2.4.1  kafka
```

#### 3、[配置环境变量](#path)

```
export KAFKA_HOME=/usr/local/src/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

#### 4)、编辑kafka下/config下/server.properties

```
broker.id=1（三台机器的这个id是不同的）
hostname=192.168.XX.XXX（当前主机的ip）
log.dirs=/usr/local/src/kafka/logs（kafka的日志目录）
zookeeper.connect=bigdata1:2181,bigdata2:2181,bigdata3:2181（zookeeper集群地址，逗号分隔）
加入kafka允许远程访问（后续通过idea调试kafka中的数据）
listeners=PLAINTEXT://0.0.0.0:9092
advertised.host.name=当前机器主机名
advertised.listeners=PLAINTEXT://192.168.XX.XXX:9092
```

#### 5)、将kafka和环境变量远程拷贝至其他机器

```
scp -r /usr/local/src/kafka/ bigdata2:/usr/local/src/
scp -r /usr/local/src/kafka/ bigdata3:/usr/local/src/

scp /root/.bash_profile bigdata2:/root/
scp /root/.bash_profile bigdata3:/root/
```

#### 6)、修改其他机器kafka上的broker.id和ip地址，主机名都对应修改好

#### 7)、三台机器分别启动kafka（先保证zookeeper是正常的）

```
kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
【说明：-daemon代表后台进程方式启动】
【kafka启动后进程就是kafka】
```

#### 8)、停止kafka服务

````
kafka-server-stop.sh
````

#### 9)、启动kafka后的节点

```
2944 QuorumPeerMain
4241 Kafka
2402 ResourceManager
2739 NodeManager
1734 NameNode
2139 SecondaryNameNode
4875 Jps
1918 DataNode
```

#### 10)、kafka的使用

- 1】创建主题

  ```
  kafka-topics.sh --create --zookeeper bigdata1:2181 --replication-factor 1 --partitions 1 --topic 主题名
  ```

-  2】查看主题

  ```
  kafka-topics.sh --list --zookeeper bigdata1:2181
  ```

- 3】删除主题

  - 说明：启动Kafaka时如果加载的配置文件中"server.properties"没有配置"delete.topic.enable=true"，那么此时的删除并不是真正的删除，而是把该topic标记为"marked for deletion"。追加参数后记得重启Kafka。

  ```
  kafka-topics.sh --delete --zookeeper bigdata1:2181  --topic 主题名
  ```

-  4】启动生产者【模拟向kafka的指定主题发送数据】

  ````
  kafka-console-producer.sh --broker-list bigdata1:9092 --topic 主题名
  ````

- 5】启动消费者【模糊查看kafka指定主题的数据】

  ```
  kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic 主题名 --from-beginning
  ```

  

### 10、<a id="hbase">Hbase</a>

#### 1)、解压

```
tar -zxvf hbase-2.2.3-bin.tar.gz -C /usr/local/src/
```

#### 2)、重命名

```
mv  hbase-2.2.3 hbase
```

#### 3)、[环境变量](#path)

```
export HBASE_HOME=/usr/local/src/hbase
export PATH=$PATH:$HBASE_HOME/bin
```

#### 4)、修改配置文件

- hbase-env.sh

  ```
  export JAVA_HOME=/usr/local/src/jdk
  export HBASE_MANAGES_ZK=false 这里改为false代表使用外部zookeeper
  ```

- hbase-site.xml

  ```
  # 说明为集群模式
  <property>
  	<name>hbase.cluster.distributed</name>
  	<value>true</value>
  </property>
  
  #  zookeeper集群的地址，不需要端口（写了也没错）
  <property>
  	<name>hbase.zookeeper.quorum</name>
  	<value>bigdata1,bigdata2,bigdata3</value>
  </property>
  
  <property>
  	<name>hbase.rootdir</name>
  	<value>hdfs://bigdata1:9000/hbase</value>
  </property>
  
  # 解决HMaster突然挂掉问题
  <property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
  </property>
  ```

- regionservers（写入datanode对应的机器节点名）

  ```
  bigdata1  
  bigdata2
  bigdata3
  ```

#### 5)、包冲突解决

```
将hbase/lib下的slf4j-reload4j-1.XXXX.jar 改个名字，后面加个bak打个标记
```

#### 6)、将hbase远程拷贝到其他两台机器上去

```
scp -r /usr/local/src/hbase/ bigdata2:/usr/local/src/
scp -r /usr/local/src/hbase/ bigdata3:/usr/local/src/
  
scp /root/.bash_profile bigdata2:/root/
source /root/.bash_profile
          
scp /root/.bash_profile bigdata3:/root/
source /root/.bash_profile
```

#### 7)、启动hbase  (前提条件：hadoop启动、zookeeper启动)

```
start-hbase.sh

停止hbase：stop-hbase.sh
```

#### 8)、查看是否配置成功

- 查看进程

  ````
  jps
  
  # bigdata1
  1746 DataNode
  1987 SecondaryNameNode
  2387 NodeManager
  8437 Jps
  3115 HMaster
  3277 HRegionServer
  
  
  # bigdata2
  1536 DataNode
  1794 QuorumPeerMain
  1940 HRegionServer
  4028 Jps
  
  # bigdata3
  1536 DataNode
  1793 QuorumPeerMain
  1933 HRegionServer
  4157 Jps
  ````
  
- 访问hbase的web页面（端口默认是16010）

  ````
  ip:16010
  ````

#### 9)、Hbase使用

- 进入hbase

  ```
  hbase shell
  ```

- 查看所有命名空间（类似于数据库概念）

  ```
  list_namespace
  ```

- 创建命名空间

  ```
  create_namespace ‘命名空间名’
  ```

- 删除命名空间

  ```
  drop_namespace "要删除的命名空间名"
  ```

- 查看所有的表

  ```
  list
  ```

- 查看某个命名空间下的表（输入  hbase shell  进入hbase命令行）

  ```
  list_namespace_tables "命名空间名"
  ```

- 创建表

  ```
  create '命名空间:表名','列族名','列族名' 
  ```

- 查看表的详细信息

  ```
  describe '表名'，或者 describe  '命名空间:表名'
  ```

- 删除表

  - 先禁用表

    ```
    disable '命名空间:表名'
    ```

  - 再删除表

    ````
    drop '命名空间:表名' 
    ````

- 向表中加入数据

  ```
  put '命名空间:表名','rowkey','列族:列名','值'
  
  put 'student1','1001','infos:name','jack'
  put 'student1','1001','infos:sex','男'
  ```

- 取出数据

  - 取出一行数据

    - 取出一行数据

      ````
      get '命名空间:表名','rowkey'
      
      get "bigdata:tb_product","1"
      ````

    - 取出一行一列数据

      ```
      get '命名空间:表名','rowkey',{COLUMN=>'列族:列名'}
      
      get "bigdata:tb_product","1",{COLUMN=>"info:name"}
      ```

    - 取出一行多列数据

      ```
      get '命名空间:表名','rowkey',{COLUMN=>['列族:列名','列族:列名']}
      
      get "bigdata:tb_product","1",{COLUMNS=>["info:name","info:price"]}
      ```

  - 取出多行数据

    - 扫描**全表**

      ```
      scan '命名空间:表名'
      
      scan "bigdata:tb_product"
      ```

    - 扫描全表**指定列**
    
      ```
      scan '命名空间:表名',{COLUMNS=>['列族:列名','列族:列名']}
      
      scan "bigdata:tb_product",{COLUMN=>"info:name"}
      ```
    
    - 全表多列
    
      ```
      scan "命名空间:表名",{COLUMNS=>["列族:列名","列族:列名"]}
      
      scan "bigdata:tb_product",{COLUMNS=>["info:name","info:price"]}
      ```
    
    - 扫描rowkey范围的**数据行**
    
      ```
      scan '命名空间:表名',{STARTROW=>'开始的rowkey',STOPROW=>'结束的rowkey'}
      
      scan 'student1',{STARTROW=>'1001',STOPROW=>'1003'}
      ```
    
    - 扫描rowkey范围的指定列数据
    
      ```
      scan '命名空间:表名',{STARTROW=>'开始的rowkey',STOPROW=>'结束的rowkey',COLUMNS=>['列族:列名']}
       
      scan 'student1',{STARTROW=>'1001',STOPROW=>'1003',COLUMNS=>['infos:name','concats:tel']}
      ```
    
    - 查看表中**前两条数据**
    
      ```
      scan '命名空间:表名',{LIMIT=>2}
      ```

  - 格式化显示(中文乱码)

    ```
    scan "bigdata:tb_product",{FORMATTER=>"toString"}
    ```

- 删除数据（同时只能删除一个Cell）

  - 删除最新版本（CELL）

    ```
    delete 'bigdata:student','1001','info:name'
    		命名空间:表名		  rowkey  列族:列名
    ```

  - 删除所有版本数据

    ```
    deleteall 'bigdata:student','1001','info:name'
    		   命名空间:表名	     rowkey  列族:列名
    ```

  - 删除一行

    ````
    deleteall 'student1','1001'
    ````
    
  - 删一列

    ```
    delete "命名空间:表名","rowkey","列族:列名"
    
    delete "bigdata:tb_product","1","info:name"
    ```

- 清空表

  ```
  truncate '命名空间:表名'
  ```

- 查看表的行数（rowkey的数量）

  ```
  count '命名空间:表名'
  ```

  

### 11、<a id="Clickhouse">Clickhouse</a>

#### 1)、解压4个clickhouse安装包到指定目录

```
tar -zxvf clickhouse-common-static-21.9.4.35.tgz -C /usr/local/src/clickhouse/
tar -zxvf clickhouse-common-static-dbg-21.9.4.35.tgz -C /usr/local/src/clickhouse/
tar -zxvf clickhouse-client-21.9.4.35.tgz -C /usr/local/src/clickhouse/
tar -zxvf clickhouse-server-21.9.4.35.tgz -C /usr/local/src/clickhouse/
```

#### 2)、进入各个目录下执行 /install/doinst.sh 执行该文件（顺序可以不管）

```
clickhouse-common-static-21.9.4.35/install/doinst.sh
clickhouse-common-static-dbg-21.9.4.35/install/doinst.sh
clickhouse-client-21.9.4.35/install/doinst.sh
clickhouse-server-21.9.4.35/install/doinst.sh
```

- 注意：其中安装clickhouse-server的时候会提示输入默认账户user的密码，和是否允许远程访问
-  注意：删除/etc/clickhouse-server/config.d/listen.xml文件

#### 3)、修改安装之后相关目录的所有权给root账户

```
chown -R root:root /var/lib/clickhouse/ /var/log/clickhouse-server/ /etc/clickhouse-server/ /etc/clickhouse-client/
```

#### 4)、配置文件（/etc/clickhouse-server/config.xml）

```
vim /etc/clickhouse-server/config.xml

1》修改远程访问
<!-- <listen_host>::</listen_host> -->
将注释去掉才能让除本机外的clickhouse访问

2》修改时区
找到timezone标签，将内容修改为Asia/Shanghai

3》修改9000端口（因为9000被hadoop给占用了）
<tcp_port>9002</tcp_port>
【注意：上面的9002为示例，也可以是9001】
【注意：最好将相关的9000端口全部给改成9002 命令模式下:%s/9000/9002】 
```

#### 5)、启动clickhouse

- 正常启动会占用终端

  ```
  clickhouse-server --config-file=/etc/clickhouse-server/config.xml
  ```

- 后台启动

  ```
  clickhouse-server --config-file=/etc/clickhouse-server/config.xml >null 2>&1 &
  【启动之后可以通过netstat -tnulp命令查看9002端口和8123端口占用情况是否正常】
  ```

  - 查看端口

    ```
    netstat -tnulp | grep clickhouse
    ```

  - 如果没有此命令（-bash: netstat: command not found）

    ```
    安装net-tools ：  yum install -y net-tools
    ```

#### 6)、连接clickhouse

```
clickhouse-client --port 9002 -u default --password 123456
【如果是远程连接其他机器上的clickhouse，可以加入参数 -h 192.168.xx.xxx】
```

#### 7)、连接其他机器clickhouse

```
 clickhouse-client -h 192.168.132.14 --port 9000 -u default --password abcd1234
```

#### 8)、使用clickhouse

- 创建数据库

  ```
  create database 数据库名;
  ```

- 定位数据库

  ```
  use 数据库名;
  ```

- 创建表（指定表的引擎）

  ```
  create table 表名(列名 类型,列名 类型,...)
  ENGINE = MergeTree()
  ORDER BY cityname
  ```

  - 引擎类别

    - TinyLog

      ```
      TinyLog表引擎是以列文件的形式保存在磁盘上，不支持索引，并且没有并发控制。一般适用于保存少量数据的小表
      ```

    - Memory

      ```
      Memory是一种内存引擎，数据会以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失，对于读写操作不会相互阻塞，但是不支持索引。简单查询下有非常高的性能表现(超过10G/s)。一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不大 (上限大概1亿行) 的场景
      ```

    - MergeTree

      ```
      ClickHouse 中最强大的表引擎当属MergeTree(合并树) 引擎及该系列( *MergeTree ) 中的其他引擎，它支持索引和分区，地位可以相当于innodb 之于Mysql。而且基于MergeTree，还衍生出了很多子引擎，是一个非常有特色的表引擎
      ```

    - ReplacingMergeTree

      ```
      ReplacingMergeTree 是 MergeTree 的一个变种，它存储特性完全继承 MergeTree，只是多了一个去重的功能。 尽管 MergeTree 可以设置主键，但是 primary key 其实没有唯一约束的功能。如果你想处理掉重复的数据，可以借助这个 ReplacingMergeTree
      ```

    - SummingMergeTree

      ```
      对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。ClickHouse 为了这种场景，提供了一种能够“预聚合”的引擎 SummingMergeTree。我们在设计聚合表的时候，可以将唯一键值、流水号去掉，所有字段全部是维度、度量或者时间戳
      ```

  - 例如

    ```
    CREATE TABLE tb_student
    (
        `sid` int,
        `sname` String,
        `birthday` date,
        `phone` String,
        `sex` String
    )
    ENGINE = MergeTree
    ORDER BY sid
    ```

- 查看表结构

  ```
  desc 表名;
  ```

  

### 12、<a id="maxwell">maxwell</a>

- maxwell监控mysql的变化
- maxwell可以用于监控mysql中表数据的变化，然后收集起来，发送给其他平台如（kafka等）

#### 1)、在mysql的配置文件  /etc/my.cnf中加入下列内容

```
原来配置的：
[mysqld]
datadir=/usr/local/src/mysql5.7/data
port=3306
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
symbolic-links=0
max_connections=400
innodb_file_per_table=1
#表名大小写不明感，敏感为
lower_case_table_names=1

#新加入的：
server-id=1
log-bin=mysql-bin
binlog_format=row
binlog-do-db=要监控的数据库名（MySQL中要有这个数据库）
```

- 注意：修改mysql的主配置文件后，要重启mysql，否则不生效

  ```
  service mysql restart
  ```

#### 3)、解压maxwell到指定的目录

```
tar -zxvf maxwell-1.29.0.tar.gz -C /usr/local/src/
```

#### 4)、重命名

```
cd /usr/local/src/
mv maxwell-1.29.0/ maxwell
```

#### 5)、[配置环境变量](#path)

```
 vim /root/.bash_profile
	 
export MAXWELL_HOME=/usr/local/src/maxwell
export PATH=$PATH:$MAXWELL_HOME/bin

source /root/.bash_profile
```

#### 6)、修改配置文件（拷贝config.properties.example命名config.properties）

```
cp config.properties.example config.properties
vim config.properties


producer=kafka
kafka.bootstrap.servers=bigdata1:9092,bigdata2:9092,bigdata3:9092
kafka_topic=要监控的数据库名

host=localhost
user=root
password=123456
```

#### 7)、分别三台机器上启动zookeeper、kafka集群

```
zkService.sh start

kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties
【说明：-daemon代表后台进程方式启动】
```

#### 8)、然后在kafka中创建主题

```
kafka-topics.sh --create --zookeeper bigdata1:2181 --replication-factor 1 --partitions 1 --topic 主题名
```

#### 9)、启动消费者【模糊查看kafka指定主题的数据】

```
kafka-console-consumer.sh --bootstrap-server bigdata3:9092 --topic 主题名 --from-beginning
```

#### 10)、启动maxwell（进入maxwell的bin目录，然后执行）

```
./maxwell --config /usr/local/src/maxwell/config.properties
```

- 启动报错（The server time zone value 'EDT' is unrecognized or represents more than one time zone）

- 在配置文件中加入如下内容

  ```
  vim config.properties
  
  添加一行:
  jdbc_options=serverTimezone=UTC 
  ```

#### 11)、常见的错误

- 启动maxwell的时候如果报server_id is '0'.

  ```
  将mysql的配置文件 /etc/my.cnf复制到/usr/local/src/mysql5.7/下，再不行，data目录下也放一个，重启mysql
  ```

  



### 13、<a id="Redis">Redis</a>



### 14、<a id="docker">Docker</a>

### 1）、基础配置：

#### 1、安装 Docker 之前，查看Centos7 Linux内核版本（官方建议 3.10 以上，3.8以上貌似也可）

```
uname -r
```

#### 2、用 root 权限更新 yum 包（出现不兼容的情况的话就必须update了）

```
yum -y update
```

- 这一步报错则说明以前安装docker时出现过错误，如果有则执行下列命令，没有则无需执行
  卸载旧版本

   ```
   yum remove docker  docker-common docker-selinux docker-engine
   ```

  

#### 3、安装docker需要的依赖， yum-util 提供yum-config-manager功能，另两个是devicemapper驱动依赖

```
yum install -y yum-utils device-mapper-persistent-data lvm2
```

#### 4、设置 yum 源（下面两个都可以，<font color="red">选择中央仓库</font>）

- 1） 第一个（中央仓库）

  ```
  yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo
  ```

- 2） 第二个（阿里云仓库）

  ```
  yum-config-manager --add-repo http://mirrors.aliyu.com/docker-ce/linux/centos/docker-ce.repo
  ```

#### 5、选择docker版本并安装

- 1）查看可用版本有哪些

  ```
  yum list docker-ce --showduplicates | sort -r
  ```

  - 如果出现什么 header selection错误，删除/etc/yum.repos.d/下的 docker-ce文件 

    ```
    删除原有的文件:
    rm -f /etc/yum.repos.d/CentOS-Base.repo
    ```

  - 重新下载

    ```
    wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
    ```

  - 清理缓存

    ```
    yum clean all
    ```



### 2）、安装Docker:

#### 1、这里我们<font color="red">直接默认安装最新版本</font>

```
yum install -y docker-ce docker-ce-cli containerd.io
```

- 选择版本安装：

  ```
  yum install docker-ce-版本号
  ```

#### 2、查看版本号

```
docker version
```

#### 3、启动docker服务

```
systemctl start docker
```

- 停止服务

  ```
  systemctl stop docker
  ```

- 开机自启

  ```
  systemctl enable docker
  ```

- 查看docker状态

  ```
  systemctl status docker
  ```



#### 3）、安装centos7：

#### 1、搜索镜像

```
docker search 镜像名
 
查询远程仓库中的centos7相关的镜像:
docker search centos7 
```

- 如果要登录仓库，登录仓库的命令是：

  ```
  docker login -u 账户名 -p 密码  远程仓库的ip
  ```

#### 2、拉取远程仓库中的镜像到本地（下面的命令不是绝对，根据比赛给予的文档参考）

```
docker pull 服务器ip/项目名/镜像名:Tag


拉取指定版本的 CentOS 镜像，这里我们安装指定版本为例(centos7)
如：
docker pull bluedata/centos7
```

#### 3、 查看当前docker中的镜像

```
docker images
```

- 删除本地镜像（根据镜像的id）

  ```
  docker rmi -f 8652b9f0cb4c
  ```



### 4）、创建一个网络:

#### 1、查看配置网络

```
docker network ls
```

#### 2、查看指定网络配置的详细信息

```
docker network inspect 网络配置名称
```

#### 3、创建网络

- 默认网络/网关

  ```
  docker network create -d bridge mynetwork2
  ```

- 指定网络/网关

  ```
  docker network create -d bridge --subnet 172.30.0.0/24 --gateway 172.30.0.1 mynetwork1
  
  
  【说明：docker创建的网段不要和宿主机一个网段，否则发生冲突】
  ```

#### 4、**查看自定**义网络

```
docker inspect mynetwork1
```

#### 5、删除网络

```
docker network rm 网络名称
```

#### 6、根据镜像创建容器

```
docker run -itd --name master --net mynetwork1 --ip 172.30.0.101 --privileged=true bluedata/centos7 /sbin/init

docker run -itd --name 容器名字 --net mynetwork --ip 172.18.0.XX --privileged=true 镜像名 /sbin/init


说明：--name后面是容器的名字
          --net是网络配置
          --ip是给容器一个固定的ip（必须是使用subnet创建的网络才能给ip）
          --privileged=true给权限，不给的话在容器中无法启动容器
          镜像名:Tag ，如果没有给Tag则默认使用Latest
          最后的 /sbin/init 不能掉
```

#### 7、查看所有容器

```
docker ps -a


说明：
-a是所有的意思，不加的话只显示Up状态的
ps只会显示正在运行Up状态的
```

#### 8、进入容器

```
docker exec -it master /bin/bash
docker exec -it 容器名 /bin/bash
或者：
docker exec -it 容器名 bash
```

- 出现错误

  ```
  Error response from daemon: Container 4d747440e5d870228864bc12b44865166348379a79630020db11e0e62906589b is not running
  ```

- 解决方法

  ````
  重启容器
  
  docker start master
  ````

  

#### 9、停止容器

```
docker stop 容器名
```

#### 10、删除容器

```
docker rm 容器名
```

#### 11、分别给三个容器主机名修改为 master、 slave1、slave2

```
vi /etc/hostname  修改文件中主机名
hostnamectl set-hostname master
```

#### 12、将需要使用的软件从虚拟机中拷贝到docker对应容器目录下

```
docker cp /opt/soft master:/opt/
docker cp /xxx 容器名:/xxx
```

#### 13、推出docker

```
ctrl + D
或
exit
```





### 













































































